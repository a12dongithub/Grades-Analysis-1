{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FinalDTUipynb.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xzZ8P29aalj",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crKiThzieR49",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import xgboost\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjVnZTgQfhwV",
        "colab_type": "text"
      },
      "source": [
        "**Loading Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Acjr43KLx9in",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "1765b1ce-f38b-49e5-87b3-bff95cce2862"
      },
      "source": [
        "vp = pd.read_csv('vpf.csv')\n",
        "vp.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Name</th>\n",
              "      <th>Year</th>\n",
              "      <th>Math1</th>\n",
              "      <th>Physics1</th>\n",
              "      <th>EE</th>\n",
              "      <th>CO</th>\n",
              "      <th>ED</th>\n",
              "      <th>FEC1</th>\n",
              "      <th>Math2</th>\n",
              "      <th>Physics2</th>\n",
              "      <th>Chem</th>\n",
              "      <th>BME</th>\n",
              "      <th>Workshop</th>\n",
              "      <th>FEC2</th>\n",
              "      <th>CGB1</th>\n",
              "      <th>CGB2</th>\n",
              "      <th>CGA1</th>\n",
              "      <th>CGA2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>kikat</td>\n",
              "      <td>2017.0</td>\n",
              "      <td>10</td>\n",
              "      <td>9</td>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "      <td>9</td>\n",
              "      <td>8</td>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "      <td>8</td>\n",
              "      <td>9.5</td>\n",
              "      <td>9.8</td>\n",
              "      <td>9.6</td>\n",
              "      <td>9.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Gegetsuburi (Zanpakutō spirit)</td>\n",
              "      <td>2017.0</td>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>9</td>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "      <td>9</td>\n",
              "      <td>7</td>\n",
              "      <td>9</td>\n",
              "      <td>9.4</td>\n",
              "      <td>9.2</td>\n",
              "      <td>9.1</td>\n",
              "      <td>9.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Hanuma</td>\n",
              "      <td>2017.0</td>\n",
              "      <td>9</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>9</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>9</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>9</td>\n",
              "      <td>7</td>\n",
              "      <td>7.7</td>\n",
              "      <td>7.8</td>\n",
              "      <td>7.9</td>\n",
              "      <td>7.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Yoshino Nara</td>\n",
              "      <td>2017.0</td>\n",
              "      <td>8</td>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "      <td>8</td>\n",
              "      <td>8.5</td>\n",
              "      <td>9.8</td>\n",
              "      <td>9.2</td>\n",
              "      <td>9.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Hibari</td>\n",
              "      <td>2017.0</td>\n",
              "      <td>9</td>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "      <td>10</td>\n",
              "      <td>9</td>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "      <td>9</td>\n",
              "      <td>8</td>\n",
              "      <td>8.8</td>\n",
              "      <td>9.5</td>\n",
              "      <td>9.3</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0                            Name    Year  ...  CGB2  CGA1  CGA2\n",
              "0           0                           kikat  2017.0  ...   9.8   9.6   9.7\n",
              "1           1  Gegetsuburi (Zanpakutō spirit)  2017.0  ...   9.2   9.1   9.5\n",
              "2           2                          Hanuma  2017.0  ...   7.8   7.9   7.6\n",
              "3           3                    Yoshino Nara  2017.0  ...   9.8   9.2   9.1\n",
              "4           4                          Hibari  2017.0  ...   9.5   9.3   9.0\n",
              "\n",
              "[5 rows x 19 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OO0S5GyFPxCF",
        "colab_type": "text"
      },
      "source": [
        "Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PniJoCJnfvHB",
        "colab_type": "text"
      },
      "source": [
        "**Functions to be used** : **plot_graphs** : used to plot graphs for epochs vs loss, \n",
        "                            **NNpredict** : used to create and train a neural 4 layered neural network(64,32,16,1 nodes respectively),\n",
        "                            **to_arr** :used to convert grades into array with numbers\n",
        "                            "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhNEbUpFL_5O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.plot(history.history['val_'+string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.legend([string, 'val_'+string])\n",
        "  plt.show()\n",
        "def NNpredict(X_train,X_test,y_train,y_test):\n",
        "  model = tf.keras.Sequential([\n",
        "                            tf.keras.layers.Dense(64, input_shape = [6], activation = 'relu'),\n",
        "                            tf.keras.layers.Dense(32, activation = 'relu'),\n",
        "                            tf.keras.layers.Dense(16, activation = 'relu'),\n",
        "                            tf.keras.layers.Dense(1)])\n",
        "  opt = tf.keras.optimizers.Adam(0.001)\n",
        "  model.compile(loss = 'mse', optimizer = opt)\n",
        "  history = model.fit(X_train,y_train, batch_size=X_train.shape[0],epochs=500, \n",
        "                    #callbacks=[tf.keras.callbacks.LearningRateScheduler(lr_scheduler)],\n",
        "                    validation_data=(X_test,y_test))\n",
        "  return model, history, sklearn.metrics.r2_score(y_test,model.predict(X_test))\n",
        "def to_arr(stri):\n",
        "  grade2idx = {'F':0,'P':1,'C':2,'B':3,'B+':4,'A':5,'A+':6,'O':7}\n",
        "  temp = stri.split(' ')\n",
        "  temp = [grade2idx[i] for i in temp]\n",
        "  return np.array(temp)  "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvcE9a0XPV7k",
        "colab_type": "text"
      },
      "source": [
        "### Model for sem1 B batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muX3vt8COq9Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gu97_jD0zE7s",
        "colab": {}
      },
      "source": [
        "X=vp[['Math1', 'Physics1', 'EE', 'CO', 'ED', 'FEC1']] # taking the semester 1 subjects for B batch from our dataset\n",
        "#Y = Final[['Math2', 'Physics2', 'Chem', 'BME', 'Workshop', 'FEC2']] this will be our predicted subjects\n",
        "Y=vp['CGB2']\n",
        "#X = X/28\n",
        "# converting everything into array and splitting our data into train and test sets\n",
        "X_trainB, X_testB, y_trainB, y_testB = train_test_split(X,Y,test_size=0.2,random_state=101)\n",
        "X_trainB = np.array(X_trainB)\n",
        "X_testB = np.array(X_testB)\n",
        "y_trainB = np.array(y_trainB)\n",
        "y_testB = np.array(y_testB)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91gFk4EdPjZt",
        "colab_type": "text"
      },
      "source": [
        "**Neural Network**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLimtRw025el",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "23bb0bba-a12f-46b9-b282-9b6d430ad221"
      },
      "source": [
        "m,h,r = NNpredict(X_trainB,X_testB,y_trainB,y_testB) # training neural network\n",
        "plot_graphs(h,'loss') # plotting loss function\n",
        "print(r) # printing r2 accuracy"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 8.6090 - val_loss: 5.7789\n",
            "Epoch 2/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 5.6093 - val_loss: 3.5778\n",
            "Epoch 3/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 3.4874 - val_loss: 2.1408\n",
            "Epoch 4/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.1089 - val_loss: 1.4203\n",
            "Epoch 5/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.4311 - val_loss: 1.3197\n",
            "Epoch 6/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.3587 - val_loss: 1.6312\n",
            "Epoch 7/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.6881 - val_loss: 2.0606\n",
            "Epoch 8/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.1245 - val_loss: 2.3623\n",
            "Epoch 9/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.4278 - val_loss: 2.4400\n",
            "Epoch 10/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 2.5038 - val_loss: 2.3157\n",
            "Epoch 11/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.3766 - val_loss: 2.0673\n",
            "Epoch 12/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.1238 - val_loss: 1.7802\n",
            "Epoch 13/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.8321 - val_loss: 1.5273\n",
            "Epoch 14/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.5722 - val_loss: 1.3521\n",
            "Epoch 15/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.3885 - val_loss: 1.2707\n",
            "Epoch 16/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.2968 - val_loss: 1.2756\n",
            "Epoch 17/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.2904 - val_loss: 1.3417\n",
            "Epoch 18/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.3455 - val_loss: 1.4350\n",
            "Epoch 19/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.4295 - val_loss: 1.5226\n",
            "Epoch 20/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.5101 - val_loss: 1.5798\n",
            "Epoch 21/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.5631 - val_loss: 1.5942\n",
            "Epoch 22/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.5763 - val_loss: 1.5662\n",
            "Epoch 23/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.5498 - val_loss: 1.5058\n",
            "Epoch 24/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.4928 - val_loss: 1.4285\n",
            "Epoch 25/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.4205 - val_loss: 1.3514\n",
            "Epoch 26/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.3491 - val_loss: 1.2886\n",
            "Epoch 27/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.2924 - val_loss: 1.2491\n",
            "Epoch 28/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.2587 - val_loss: 1.2353\n",
            "Epoch 29/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.2501 - val_loss: 1.2433\n",
            "Epoch 30/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.2620 - val_loss: 1.2642\n",
            "Epoch 31/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.2857 - val_loss: 1.2875\n",
            "Epoch 32/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.3104 - val_loss: 1.3037\n",
            "Epoch 33/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.3267 - val_loss: 1.3070\n",
            "Epoch 34/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.3291 - val_loss: 1.2963\n",
            "Epoch 35/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.3169 - val_loss: 1.2757\n",
            "Epoch 36/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.2939 - val_loss: 1.2516\n",
            "Epoch 37/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.2667 - val_loss: 1.2304\n",
            "Epoch 38/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.2421 - val_loss: 1.2167\n",
            "Epoch 39/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.2248 - val_loss: 1.2122\n",
            "Epoch 40/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.2167 - val_loss: 1.2155\n",
            "Epoch 41/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.2165 - val_loss: 1.2228\n",
            "Epoch 42/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.2208 - val_loss: 1.2300\n",
            "Epoch 43/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.2255 - val_loss: 1.2335\n",
            "Epoch 44/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.2272 - val_loss: 1.2312\n",
            "Epoch 45/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.2239 - val_loss: 1.2234\n",
            "Epoch 46/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.2160 - val_loss: 1.2125\n",
            "Epoch 47/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.2053 - val_loss: 1.2016\n",
            "Epoch 48/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 1.1949 - val_loss: 1.1946\n",
            "Epoch 49/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.1878 - val_loss: 1.1927\n",
            "Epoch 50/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.1856 - val_loss: 1.1954\n",
            "Epoch 51/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.1882 - val_loss: 1.2000\n",
            "Epoch 52/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.1923 - val_loss: 1.2028\n",
            "Epoch 53/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.1943 - val_loss: 1.2019\n",
            "Epoch 54/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.1922 - val_loss: 1.1978\n",
            "Epoch 55/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.1870 - val_loss: 1.1930\n",
            "Epoch 56/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.1811 - val_loss: 1.1897\n",
            "Epoch 57/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.1767 - val_loss: 1.1890\n",
            "Epoch 58/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.1750 - val_loss: 1.1903\n",
            "Epoch 59/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.1754 - val_loss: 1.1925\n",
            "Epoch 60/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.1766 - val_loss: 1.1937\n",
            "Epoch 61/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.1770 - val_loss: 1.1929\n",
            "Epoch 62/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.1755 - val_loss: 1.1905\n",
            "Epoch 63/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.1727 - val_loss: 1.1878\n",
            "Epoch 64/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.1694 - val_loss: 1.1860\n",
            "Epoch 65/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.1669 - val_loss: 1.1855\n",
            "Epoch 66/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.1659 - val_loss: 1.1858\n",
            "Epoch 67/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.1656 - val_loss: 1.1861\n",
            "Epoch 68/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.1653 - val_loss: 1.1856\n",
            "Epoch 69/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.1643 - val_loss: 1.1841\n",
            "Epoch 70/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.1624 - val_loss: 1.1824\n",
            "Epoch 71/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.1602 - val_loss: 1.1814\n",
            "Epoch 72/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.1583 - val_loss: 1.1808\n",
            "Epoch 73/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.1570 - val_loss: 1.1807\n",
            "Epoch 74/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.1563 - val_loss: 1.1805\n",
            "Epoch 75/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.1556 - val_loss: 1.1801\n",
            "Epoch 76/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.1545 - val_loss: 1.1791\n",
            "Epoch 77/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.1530 - val_loss: 1.1777\n",
            "Epoch 78/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.1513 - val_loss: 1.1766\n",
            "Epoch 79/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.1497 - val_loss: 1.1758\n",
            "Epoch 80/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.1485 - val_loss: 1.1748\n",
            "Epoch 81/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.1477 - val_loss: 1.1741\n",
            "Epoch 82/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.1470 - val_loss: 1.1734\n",
            "Epoch 83/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.1459 - val_loss: 1.1729\n",
            "Epoch 84/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.1445 - val_loss: 1.1726\n",
            "Epoch 85/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.1431 - val_loss: 1.1728\n",
            "Epoch 86/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.1420 - val_loss: 1.1731\n",
            "Epoch 87/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.1413 - val_loss: 1.1729\n",
            "Epoch 88/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.1406 - val_loss: 1.1721\n",
            "Epoch 89/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.1395 - val_loss: 1.1708\n",
            "Epoch 90/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.1382 - val_loss: 1.1695\n",
            "Epoch 91/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.1371 - val_loss: 1.1685\n",
            "Epoch 92/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.1363 - val_loss: 1.1677\n",
            "Epoch 93/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.1356 - val_loss: 1.1670\n",
            "Epoch 94/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.1348 - val_loss: 1.1664\n",
            "Epoch 95/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.1338 - val_loss: 1.1662\n",
            "Epoch 96/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.1328 - val_loss: 1.1663\n",
            "Epoch 97/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.1320 - val_loss: 1.1665\n",
            "Epoch 98/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.1313 - val_loss: 1.1663\n",
            "Epoch 99/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.1306 - val_loss: 1.1658\n",
            "Epoch 100/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.1297 - val_loss: 1.1653\n",
            "Epoch 101/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.1288 - val_loss: 1.1650\n",
            "Epoch 102/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.1281 - val_loss: 1.1650\n",
            "Epoch 103/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.1275 - val_loss: 1.1650\n",
            "Epoch 104/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.1267 - val_loss: 1.1650\n",
            "Epoch 105/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.1259 - val_loss: 1.1649\n",
            "Epoch 106/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.1252 - val_loss: 1.1647\n",
            "Epoch 107/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.1246 - val_loss: 1.1643\n",
            "Epoch 108/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.1239 - val_loss: 1.1637\n",
            "Epoch 109/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.1232 - val_loss: 1.1630\n",
            "Epoch 110/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.1225 - val_loss: 1.1625\n",
            "Epoch 111/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.1218 - val_loss: 1.1620\n",
            "Epoch 112/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.1211 - val_loss: 1.1616\n",
            "Epoch 113/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.1204 - val_loss: 1.1612\n",
            "Epoch 114/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.1197 - val_loss: 1.1607\n",
            "Epoch 115/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.1191 - val_loss: 1.1602\n",
            "Epoch 116/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.1184 - val_loss: 1.1597\n",
            "Epoch 117/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.1178 - val_loss: 1.1593\n",
            "Epoch 118/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.1172 - val_loss: 1.1589\n",
            "Epoch 119/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.1165 - val_loss: 1.1586\n",
            "Epoch 120/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.1159 - val_loss: 1.1583\n",
            "Epoch 121/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.1153 - val_loss: 1.1578\n",
            "Epoch 122/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.1147 - val_loss: 1.1573\n",
            "Epoch 123/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.1141 - val_loss: 1.1571\n",
            "Epoch 124/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.1135 - val_loss: 1.1569\n",
            "Epoch 125/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.1129 - val_loss: 1.1565\n",
            "Epoch 126/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.1124 - val_loss: 1.1563\n",
            "Epoch 127/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.1118 - val_loss: 1.1562\n",
            "Epoch 128/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.1113 - val_loss: 1.1562\n",
            "Epoch 129/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.1107 - val_loss: 1.1561\n",
            "Epoch 130/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.1102 - val_loss: 1.1558\n",
            "Epoch 131/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.1096 - val_loss: 1.1554\n",
            "Epoch 132/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.1091 - val_loss: 1.1550\n",
            "Epoch 133/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.1085 - val_loss: 1.1549\n",
            "Epoch 134/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.1080 - val_loss: 1.1552\n",
            "Epoch 135/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.1075 - val_loss: 1.1553\n",
            "Epoch 136/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.1069 - val_loss: 1.1552\n",
            "Epoch 137/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.1064 - val_loss: 1.1549\n",
            "Epoch 138/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.1059 - val_loss: 1.1545\n",
            "Epoch 139/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.1054 - val_loss: 1.1541\n",
            "Epoch 140/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.1048 - val_loss: 1.1538\n",
            "Epoch 141/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.1043 - val_loss: 1.1534\n",
            "Epoch 142/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.1038 - val_loss: 1.1530\n",
            "Epoch 143/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.1033 - val_loss: 1.1532\n",
            "Epoch 144/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.1028 - val_loss: 1.1529\n",
            "Epoch 145/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 1.1023 - val_loss: 1.1524\n",
            "Epoch 146/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.1018 - val_loss: 1.1518\n",
            "Epoch 147/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.1014 - val_loss: 1.1520\n",
            "Epoch 148/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.1008 - val_loss: 1.1522\n",
            "Epoch 149/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.1004 - val_loss: 1.1516\n",
            "Epoch 150/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0998 - val_loss: 1.1513\n",
            "Epoch 151/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0993 - val_loss: 1.1513\n",
            "Epoch 152/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0989 - val_loss: 1.1511\n",
            "Epoch 153/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0984 - val_loss: 1.1506\n",
            "Epoch 154/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.0979 - val_loss: 1.1504\n",
            "Epoch 155/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0974 - val_loss: 1.1505\n",
            "Epoch 156/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0970 - val_loss: 1.1498\n",
            "Epoch 157/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0965 - val_loss: 1.1495\n",
            "Epoch 158/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.0961 - val_loss: 1.1498\n",
            "Epoch 159/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0956 - val_loss: 1.1494\n",
            "Epoch 160/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0951 - val_loss: 1.1492\n",
            "Epoch 161/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0947 - val_loss: 1.1490\n",
            "Epoch 162/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0942 - val_loss: 1.1481\n",
            "Epoch 163/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 1.0938 - val_loss: 1.1478\n",
            "Epoch 164/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0933 - val_loss: 1.1480\n",
            "Epoch 165/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0928 - val_loss: 1.1469\n",
            "Epoch 166/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0924 - val_loss: 1.1465\n",
            "Epoch 167/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0919 - val_loss: 1.1466\n",
            "Epoch 168/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0915 - val_loss: 1.1471\n",
            "Epoch 169/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0910 - val_loss: 1.1462\n",
            "Epoch 170/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0905 - val_loss: 1.1455\n",
            "Epoch 171/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0901 - val_loss: 1.1456\n",
            "Epoch 172/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0896 - val_loss: 1.1458\n",
            "Epoch 173/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0892 - val_loss: 1.1457\n",
            "Epoch 174/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0887 - val_loss: 1.1447\n",
            "Epoch 175/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0883 - val_loss: 1.1440\n",
            "Epoch 176/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0878 - val_loss: 1.1434\n",
            "Epoch 177/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0874 - val_loss: 1.1429\n",
            "Epoch 178/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0869 - val_loss: 1.1424\n",
            "Epoch 179/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0865 - val_loss: 1.1420\n",
            "Epoch 180/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0860 - val_loss: 1.1421\n",
            "Epoch 181/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0855 - val_loss: 1.1418\n",
            "Epoch 182/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0851 - val_loss: 1.1412\n",
            "Epoch 183/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.0846 - val_loss: 1.1406\n",
            "Epoch 184/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.0842 - val_loss: 1.1403\n",
            "Epoch 185/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0837 - val_loss: 1.1403\n",
            "Epoch 186/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0833 - val_loss: 1.1401\n",
            "Epoch 187/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.0828 - val_loss: 1.1395\n",
            "Epoch 188/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0823 - val_loss: 1.1389\n",
            "Epoch 189/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0819 - val_loss: 1.1383\n",
            "Epoch 190/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0814 - val_loss: 1.1377\n",
            "Epoch 191/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0810 - val_loss: 1.1379\n",
            "Epoch 192/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0805 - val_loss: 1.1376\n",
            "Epoch 193/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0800 - val_loss: 1.1370\n",
            "Epoch 194/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0796 - val_loss: 1.1369\n",
            "Epoch 195/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0791 - val_loss: 1.1366\n",
            "Epoch 196/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0786 - val_loss: 1.1360\n",
            "Epoch 197/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0781 - val_loss: 1.1352\n",
            "Epoch 198/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0777 - val_loss: 1.1346\n",
            "Epoch 199/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0772 - val_loss: 1.1344\n",
            "Epoch 200/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0767 - val_loss: 1.1344\n",
            "Epoch 201/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.0761 - val_loss: 1.1341\n",
            "Epoch 202/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0756 - val_loss: 1.1337\n",
            "Epoch 203/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0751 - val_loss: 1.1334\n",
            "Epoch 204/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0746 - val_loss: 1.1336\n",
            "Epoch 205/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0741 - val_loss: 1.1328\n",
            "Epoch 206/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.0736 - val_loss: 1.1321\n",
            "Epoch 207/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0731 - val_loss: 1.1318\n",
            "Epoch 208/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.0726 - val_loss: 1.1317\n",
            "Epoch 209/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0721 - val_loss: 1.1313\n",
            "Epoch 210/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.0716 - val_loss: 1.1309\n",
            "Epoch 211/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.0711 - val_loss: 1.1312\n",
            "Epoch 212/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.0706 - val_loss: 1.1312\n",
            "Epoch 213/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.0701 - val_loss: 1.1305\n",
            "Epoch 214/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0696 - val_loss: 1.1298\n",
            "Epoch 215/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0692 - val_loss: 1.1301\n",
            "Epoch 216/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0687 - val_loss: 1.1294\n",
            "Epoch 217/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0682 - val_loss: 1.1282\n",
            "Epoch 218/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.0677 - val_loss: 1.1268\n",
            "Epoch 219/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.0672 - val_loss: 1.1260\n",
            "Epoch 220/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.0667 - val_loss: 1.1259\n",
            "Epoch 221/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.0662 - val_loss: 1.1256\n",
            "Epoch 222/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0657 - val_loss: 1.1248\n",
            "Epoch 223/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0652 - val_loss: 1.1247\n",
            "Epoch 224/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0647 - val_loss: 1.1248\n",
            "Epoch 225/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0642 - val_loss: 1.1239\n",
            "Epoch 226/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0636 - val_loss: 1.1228\n",
            "Epoch 227/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.0631 - val_loss: 1.1223\n",
            "Epoch 228/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0626 - val_loss: 1.1223\n",
            "Epoch 229/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0621 - val_loss: 1.1219\n",
            "Epoch 230/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0615 - val_loss: 1.1212\n",
            "Epoch 231/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0610 - val_loss: 1.1207\n",
            "Epoch 232/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0605 - val_loss: 1.1208\n",
            "Epoch 233/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0599 - val_loss: 1.1206\n",
            "Epoch 234/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0595 - val_loss: 1.1198\n",
            "Epoch 235/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0589 - val_loss: 1.1191\n",
            "Epoch 236/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0584 - val_loss: 1.1190\n",
            "Epoch 237/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.0579 - val_loss: 1.1180\n",
            "Epoch 238/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0573 - val_loss: 1.1172\n",
            "Epoch 239/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0567 - val_loss: 1.1160\n",
            "Epoch 240/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.0561 - val_loss: 1.1156\n",
            "Epoch 241/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.0555 - val_loss: 1.1146\n",
            "Epoch 242/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0549 - val_loss: 1.1136\n",
            "Epoch 243/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0544 - val_loss: 1.1144\n",
            "Epoch 244/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0538 - val_loss: 1.1146\n",
            "Epoch 245/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0532 - val_loss: 1.1127\n",
            "Epoch 246/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0529 - val_loss: 1.1136\n",
            "Epoch 247/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0521 - val_loss: 1.1140\n",
            "Epoch 248/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0516 - val_loss: 1.1112\n",
            "Epoch 249/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0509 - val_loss: 1.1108\n",
            "Epoch 250/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0502 - val_loss: 1.1111\n",
            "Epoch 251/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0497 - val_loss: 1.1098\n",
            "Epoch 252/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.0489 - val_loss: 1.1092\n",
            "Epoch 253/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0483 - val_loss: 1.1106\n",
            "Epoch 254/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0477 - val_loss: 1.1089\n",
            "Epoch 255/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0471 - val_loss: 1.1093\n",
            "Epoch 256/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0464 - val_loss: 1.1097\n",
            "Epoch 257/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0458 - val_loss: 1.1079\n",
            "Epoch 258/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0451 - val_loss: 1.1080\n",
            "Epoch 259/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0444 - val_loss: 1.1081\n",
            "Epoch 260/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0439 - val_loss: 1.1079\n",
            "Epoch 261/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0432 - val_loss: 1.1077\n",
            "Epoch 262/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0426 - val_loss: 1.1077\n",
            "Epoch 263/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0421 - val_loss: 1.1074\n",
            "Epoch 264/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.0414 - val_loss: 1.1070\n",
            "Epoch 265/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.0408 - val_loss: 1.1060\n",
            "Epoch 266/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.0402 - val_loss: 1.1061\n",
            "Epoch 267/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0397 - val_loss: 1.1050\n",
            "Epoch 268/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.0390 - val_loss: 1.1049\n",
            "Epoch 269/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0384 - val_loss: 1.1051\n",
            "Epoch 270/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0378 - val_loss: 1.1044\n",
            "Epoch 271/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0372 - val_loss: 1.1045\n",
            "Epoch 272/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.0367 - val_loss: 1.1037\n",
            "Epoch 273/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.0361 - val_loss: 1.1027\n",
            "Epoch 274/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0354 - val_loss: 1.1031\n",
            "Epoch 275/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0349 - val_loss: 1.1021\n",
            "Epoch 276/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.0342 - val_loss: 1.1014\n",
            "Epoch 277/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0337 - val_loss: 1.1021\n",
            "Epoch 278/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.0331 - val_loss: 1.1003\n",
            "Epoch 279/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0327 - val_loss: 1.1010\n",
            "Epoch 280/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.0320 - val_loss: 1.0998\n",
            "Epoch 281/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.0314 - val_loss: 1.0990\n",
            "Epoch 282/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.0308 - val_loss: 1.0993\n",
            "Epoch 283/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0302 - val_loss: 1.0983\n",
            "Epoch 284/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.0296 - val_loss: 1.0986\n",
            "Epoch 285/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.0290 - val_loss: 1.0982\n",
            "Epoch 286/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.0285 - val_loss: 1.0978\n",
            "Epoch 287/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0280 - val_loss: 1.0976\n",
            "Epoch 288/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.0274 - val_loss: 1.0964\n",
            "Epoch 289/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.0268 - val_loss: 1.0965\n",
            "Epoch 290/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.0262 - val_loss: 1.0949\n",
            "Epoch 291/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.0256 - val_loss: 1.0943\n",
            "Epoch 292/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0250 - val_loss: 1.0926\n",
            "Epoch 293/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0242 - val_loss: 1.0915\n",
            "Epoch 294/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0235 - val_loss: 1.0910\n",
            "Epoch 295/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0228 - val_loss: 1.0905\n",
            "Epoch 296/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0221 - val_loss: 1.0896\n",
            "Epoch 297/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0214 - val_loss: 1.0902\n",
            "Epoch 298/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0206 - val_loss: 1.0884\n",
            "Epoch 299/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0200 - val_loss: 1.0897\n",
            "Epoch 300/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0195 - val_loss: 1.0861\n",
            "Epoch 301/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.0193 - val_loss: 1.0892\n",
            "Epoch 302/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0184 - val_loss: 1.0847\n",
            "Epoch 303/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0173 - val_loss: 1.0841\n",
            "Epoch 304/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0166 - val_loss: 1.0858\n",
            "Epoch 305/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0163 - val_loss: 1.0803\n",
            "Epoch 306/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.0162 - val_loss: 1.0843\n",
            "Epoch 307/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.0152 - val_loss: 1.0814\n",
            "Epoch 308/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 1.0141 - val_loss: 1.0786\n",
            "Epoch 309/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.0148 - val_loss: 1.0856\n",
            "Epoch 310/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0146 - val_loss: 1.0782\n",
            "Epoch 311/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0124 - val_loss: 1.0764\n",
            "Epoch 312/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0126 - val_loss: 1.0810\n",
            "Epoch 313/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0122 - val_loss: 1.0760\n",
            "Epoch 314/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0107 - val_loss: 1.0738\n",
            "Epoch 315/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.0110 - val_loss: 1.0773\n",
            "Epoch 316/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0103 - val_loss: 1.0747\n",
            "Epoch 317/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0092 - val_loss: 1.0716\n",
            "Epoch 318/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.0096 - val_loss: 1.0737\n",
            "Epoch 319/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.0083 - val_loss: 1.0730\n",
            "Epoch 320/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 1.0078 - val_loss: 1.0696\n",
            "Epoch 321/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0078 - val_loss: 1.0712\n",
            "Epoch 322/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.0067 - val_loss: 1.0715\n",
            "Epoch 323/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0064 - val_loss: 1.0680\n",
            "Epoch 324/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0061 - val_loss: 1.0687\n",
            "Epoch 325/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.0052 - val_loss: 1.0698\n",
            "Epoch 326/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.0051 - val_loss: 1.0662\n",
            "Epoch 327/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0047 - val_loss: 1.0664\n",
            "Epoch 328/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.0038 - val_loss: 1.0674\n",
            "Epoch 329/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0037 - val_loss: 1.0641\n",
            "Epoch 330/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0032 - val_loss: 1.0640\n",
            "Epoch 331/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0026 - val_loss: 1.0654\n",
            "Epoch 332/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.0025 - val_loss: 1.0624\n",
            "Epoch 333/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.0019 - val_loss: 1.0621\n",
            "Epoch 334/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.0014 - val_loss: 1.0636\n",
            "Epoch 335/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0012 - val_loss: 1.0612\n",
            "Epoch 336/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 1.0006 - val_loss: 1.0609\n",
            "Epoch 337/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.0002 - val_loss: 1.0623\n",
            "Epoch 338/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.0000 - val_loss: 1.0601\n",
            "Epoch 339/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9994 - val_loss: 1.0586\n",
            "Epoch 340/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.9991 - val_loss: 1.0596\n",
            "Epoch 341/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9989 - val_loss: 1.0573\n",
            "Epoch 342/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.9983 - val_loss: 1.0560\n",
            "Epoch 343/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9981 - val_loss: 1.0582\n",
            "Epoch 344/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.9979 - val_loss: 1.0563\n",
            "Epoch 345/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.9973 - val_loss: 1.0549\n",
            "Epoch 346/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.9971 - val_loss: 1.0570\n",
            "Epoch 347/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.9967 - val_loss: 1.0552\n",
            "Epoch 348/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.9962 - val_loss: 1.0535\n",
            "Epoch 349/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9960 - val_loss: 1.0551\n",
            "Epoch 350/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.9956 - val_loss: 1.0530\n",
            "Epoch 351/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9951 - val_loss: 1.0512\n",
            "Epoch 352/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.9949 - val_loss: 1.0531\n",
            "Epoch 353/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.9947 - val_loss: 1.0498\n",
            "Epoch 354/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9942 - val_loss: 1.0505\n",
            "Epoch 355/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.9938 - val_loss: 1.0510\n",
            "Epoch 356/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.9935 - val_loss: 1.0492\n",
            "Epoch 357/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.9932 - val_loss: 1.0511\n",
            "Epoch 358/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.9930 - val_loss: 1.0482\n",
            "Epoch 359/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9927 - val_loss: 1.0498\n",
            "Epoch 360/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.9924 - val_loss: 1.0479\n",
            "Epoch 361/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9920 - val_loss: 1.0482\n",
            "Epoch 362/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.9917 - val_loss: 1.0479\n",
            "Epoch 363/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9914 - val_loss: 1.0461\n",
            "Epoch 364/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.9911 - val_loss: 1.0466\n",
            "Epoch 365/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.9909 - val_loss: 1.0449\n",
            "Epoch 366/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.9906 - val_loss: 1.0450\n",
            "Epoch 367/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.9903 - val_loss: 1.0451\n",
            "Epoch 368/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9900 - val_loss: 1.0443\n",
            "Epoch 369/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9897 - val_loss: 1.0450\n",
            "Epoch 370/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.9896 - val_loss: 1.0439\n",
            "Epoch 371/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.9893 - val_loss: 1.0440\n",
            "Epoch 372/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.9889 - val_loss: 1.0429\n",
            "Epoch 373/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.9887 - val_loss: 1.0415\n",
            "Epoch 374/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.9885 - val_loss: 1.0431\n",
            "Epoch 375/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.9883 - val_loss: 1.0402\n",
            "Epoch 376/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9881 - val_loss: 1.0432\n",
            "Epoch 377/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.9879 - val_loss: 1.0400\n",
            "Epoch 378/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.9877 - val_loss: 1.0411\n",
            "Epoch 379/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.9873 - val_loss: 1.0397\n",
            "Epoch 380/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9870 - val_loss: 1.0381\n",
            "Epoch 381/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.9868 - val_loss: 1.0399\n",
            "Epoch 382/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.9867 - val_loss: 1.0375\n",
            "Epoch 383/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9864 - val_loss: 1.0389\n",
            "Epoch 384/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.9861 - val_loss: 1.0379\n",
            "Epoch 385/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.9858 - val_loss: 1.0367\n",
            "Epoch 386/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9856 - val_loss: 1.0383\n",
            "Epoch 387/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.9855 - val_loss: 1.0343\n",
            "Epoch 388/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.9854 - val_loss: 1.0390\n",
            "Epoch 389/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9856 - val_loss: 1.0329\n",
            "Epoch 390/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.9859 - val_loss: 1.0406\n",
            "Epoch 391/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.9858 - val_loss: 1.0339\n",
            "Epoch 392/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.9846 - val_loss: 1.0355\n",
            "Epoch 393/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9841 - val_loss: 1.0382\n",
            "Epoch 394/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.9844 - val_loss: 1.0320\n",
            "Epoch 395/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.9846 - val_loss: 1.0378\n",
            "Epoch 396/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.9842 - val_loss: 1.0325\n",
            "Epoch 397/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.9833 - val_loss: 1.0317\n",
            "Epoch 398/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9833 - val_loss: 1.0359\n",
            "Epoch 399/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.9835 - val_loss: 1.0298\n",
            "Epoch 400/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9834 - val_loss: 1.0334\n",
            "Epoch 401/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.9827 - val_loss: 1.0326\n",
            "Epoch 402/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.9824 - val_loss: 1.0300\n",
            "Epoch 403/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.9825 - val_loss: 1.0333\n",
            "Epoch 404/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.9823 - val_loss: 1.0299\n",
            "Epoch 405/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.9819 - val_loss: 1.0300\n",
            "Epoch 406/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.9816 - val_loss: 1.0318\n",
            "Epoch 407/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9816 - val_loss: 1.0288\n",
            "Epoch 408/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9816 - val_loss: 1.0315\n",
            "Epoch 409/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9813 - val_loss: 1.0288\n",
            "Epoch 410/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9810 - val_loss: 1.0282\n",
            "Epoch 411/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.9808 - val_loss: 1.0298\n",
            "Epoch 412/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.9808 - val_loss: 1.0271\n",
            "Epoch 413/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.9806 - val_loss: 1.0288\n",
            "Epoch 414/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.9803 - val_loss: 1.0286\n",
            "Epoch 415/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.9801 - val_loss: 1.0271\n",
            "Epoch 416/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.9800 - val_loss: 1.0289\n",
            "Epoch 417/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.9800 - val_loss: 1.0258\n",
            "Epoch 418/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.9797 - val_loss: 1.0269\n",
            "Epoch 419/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.9795 - val_loss: 1.0272\n",
            "Epoch 420/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.9793 - val_loss: 1.0255\n",
            "Epoch 421/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.9792 - val_loss: 1.0279\n",
            "Epoch 422/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.9791 - val_loss: 1.0250\n",
            "Epoch 423/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.9789 - val_loss: 1.0267\n",
            "Epoch 424/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.9787 - val_loss: 1.0247\n",
            "Epoch 425/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9785 - val_loss: 1.0245\n",
            "Epoch 426/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.9783 - val_loss: 1.0251\n",
            "Epoch 427/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.9782 - val_loss: 1.0237\n",
            "Epoch 428/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.9780 - val_loss: 1.0242\n",
            "Epoch 429/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9778 - val_loss: 1.0238\n",
            "Epoch 430/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.9777 - val_loss: 1.0230\n",
            "Epoch 431/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.9775 - val_loss: 1.0235\n",
            "Epoch 432/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.9774 - val_loss: 1.0219\n",
            "Epoch 433/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.9774 - val_loss: 1.0251\n",
            "Epoch 434/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.9775 - val_loss: 1.0206\n",
            "Epoch 435/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.9775 - val_loss: 1.0251\n",
            "Epoch 436/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9774 - val_loss: 1.0202\n",
            "Epoch 437/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9770 - val_loss: 1.0224\n",
            "Epoch 438/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.9766 - val_loss: 1.0212\n",
            "Epoch 439/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.9764 - val_loss: 1.0212\n",
            "Epoch 440/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.9763 - val_loss: 1.0220\n",
            "Epoch 441/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.9762 - val_loss: 1.0190\n",
            "Epoch 442/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9763 - val_loss: 1.0223\n",
            "Epoch 443/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.9763 - val_loss: 1.0182\n",
            "Epoch 444/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9761 - val_loss: 1.0215\n",
            "Epoch 445/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9758 - val_loss: 1.0191\n",
            "Epoch 446/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9756 - val_loss: 1.0204\n",
            "Epoch 447/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.9754 - val_loss: 1.0203\n",
            "Epoch 448/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.9752 - val_loss: 1.0183\n",
            "Epoch 449/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.9752 - val_loss: 1.0209\n",
            "Epoch 450/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.9752 - val_loss: 1.0176\n",
            "Epoch 451/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9752 - val_loss: 1.0210\n",
            "Epoch 452/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9749 - val_loss: 1.0179\n",
            "Epoch 453/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.9748 - val_loss: 1.0192\n",
            "Epoch 454/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9745 - val_loss: 1.0183\n",
            "Epoch 455/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9744 - val_loss: 1.0181\n",
            "Epoch 456/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.9743 - val_loss: 1.0187\n",
            "Epoch 457/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.9742 - val_loss: 1.0187\n",
            "Epoch 458/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9740 - val_loss: 1.0170\n",
            "Epoch 459/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9740 - val_loss: 1.0191\n",
            "Epoch 460/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9740 - val_loss: 1.0154\n",
            "Epoch 461/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9741 - val_loss: 1.0202\n",
            "Epoch 462/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9741 - val_loss: 1.0155\n",
            "Epoch 463/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.9740 - val_loss: 1.0198\n",
            "Epoch 464/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.9738 - val_loss: 1.0162\n",
            "Epoch 465/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.9734 - val_loss: 1.0171\n",
            "Epoch 466/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.9732 - val_loss: 1.0177\n",
            "Epoch 467/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9731 - val_loss: 1.0152\n",
            "Epoch 468/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9733 - val_loss: 1.0201\n",
            "Epoch 469/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.9736 - val_loss: 1.0142\n",
            "Epoch 470/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9732 - val_loss: 1.0166\n",
            "Epoch 471/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.9727 - val_loss: 1.0163\n",
            "Epoch 472/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.9726 - val_loss: 1.0145\n",
            "Epoch 473/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.9727 - val_loss: 1.0185\n",
            "Epoch 474/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9728 - val_loss: 1.0139\n",
            "Epoch 475/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9725 - val_loss: 1.0162\n",
            "Epoch 476/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.9722 - val_loss: 1.0149\n",
            "Epoch 477/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9720 - val_loss: 1.0135\n",
            "Epoch 478/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9720 - val_loss: 1.0170\n",
            "Epoch 479/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.9722 - val_loss: 1.0130\n",
            "Epoch 480/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.9721 - val_loss: 1.0159\n",
            "Epoch 481/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.9717 - val_loss: 1.0142\n",
            "Epoch 482/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9715 - val_loss: 1.0128\n",
            "Epoch 483/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9716 - val_loss: 1.0171\n",
            "Epoch 484/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9719 - val_loss: 1.0122\n",
            "Epoch 485/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9716 - val_loss: 1.0150\n",
            "Epoch 486/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9712 - val_loss: 1.0137\n",
            "Epoch 487/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.9709 - val_loss: 1.0113\n",
            "Epoch 488/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.9711 - val_loss: 1.0163\n",
            "Epoch 489/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.9715 - val_loss: 1.0110\n",
            "Epoch 490/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.9713 - val_loss: 1.0147\n",
            "Epoch 491/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.9708 - val_loss: 1.0128\n",
            "Epoch 492/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.9704 - val_loss: 1.0114\n",
            "Epoch 493/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.9706 - val_loss: 1.0168\n",
            "Epoch 494/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.9711 - val_loss: 1.0108\n",
            "Epoch 495/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.9706 - val_loss: 1.0133\n",
            "Epoch 496/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.9701 - val_loss: 1.0124\n",
            "Epoch 497/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.9699 - val_loss: 1.0110\n",
            "Epoch 498/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.9699 - val_loss: 1.0133\n",
            "Epoch 499/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.9699 - val_loss: 1.0100\n",
            "Epoch 500/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.9698 - val_loss: 1.0128\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEGCAYAAABvtY4XAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de5hcVZnv8e+7d1V1d5JOAqGTgImEPCJREgGnQRgBBUdURuV4jQgKjMpz0AfwMijqeAYdfJwRHxwdPXAyeDcqGcA5HnFARjJcZjDSCQGCCEgEpsMlnZhbJ+nuqr3f88feVV3p7sTuTu+u7t2/z0M9+1K79lqr0rxr1dprr23ujoiI5E/Q6AyIiEg2FOBFRHJKAV5EJKcU4EVEckoBXkQkpwqNzkC9ww47zBctWtTobIiITBpr167d4u5tQ703oQL8okWL6OjoaHQ2REQmDTN7en/vqYtGRCSnFOBFRHJKAV5EJKcmVB+8iEw95XKZzs5Oenp6Gp2VCa25uZkFCxZQLBaH/RkFeBFpqM7OTlpbW1m0aBFm1ujsTEjuztatW+ns7OSoo44a9ufURSMiDdXT08OcOXMU3A/AzJgzZ86If+UowItIwym4/2mj+Y5yEeD/6VdPcNfjXY3OhojIhJKLAH/dXU9y7xMK8CIyOjNmzGh0FjKRiwAfmBHruSUiIvvIRYA3g1hPphKRg+TuXHHFFSxdupRly5Zx4403AvDcc89x+umnc/zxx7N06VLuueceoijiwgsvrB371a9+tcG5HywXwyQDMxTfRSa/z/+/R/jtszvH9JwvP2Imf/uWY4d17C233ML69et58MEH2bJlCyeeeCKnn346P/rRj3jDG97AZz/7WaIoYs+ePaxfv55NmzaxYcMGALZv3z6m+R4LuWjBB2rBi8gYuPfeezn33HMJw5B58+bxmte8hvvvv58TTzyR73znO1x11VU8/PDDtLa2snjxYjZu3Mill17KbbfdxsyZMxud/UFy04JXgBeZ/Ibb0h5vp59+OnfffTe33norF154IR//+Md5//vfz4MPPsjtt9/O9ddfz6pVq/j2t7/d6KzuIxcteNNFVhEZA6eddho33ngjURTR1dXF3XffzUknncTTTz/NvHnz+NCHPsQHP/hB1q1bx5YtW4jjmHe84x1cffXVrFu3rtHZHyQnLfjk4oiIyMF429vexn333cdxxx2HmfHlL3+Z+fPn873vfY9rrrmGYrHIjBkz+P73v8+mTZu46KKLiOMYgC996UsNzv1gOQnwRvodi4iMWHd3N5D0BlxzzTVcc801+7x/wQUXcMEFFwz63ERstdfLRReNLrKKiAyWiwCvPngRkcFyEeCDQH3wIiID5SPAa5ikiMggmQZ4M/uYmT1iZhvM7Mdm1pxFOpqLRkRksMwCvJm9CLgMaHf3pUAIvCebtHSRVURkoKy7aApAi5kVgGnAs1kkorloREQGyyzAu/sm4CvAM8BzwA53/+XA48zsYjPrMLOOrq7RzemuYZIiMl4ONHf8U089xdKlS8cxNweWZRfNIcA5wFHAEcB0Mzt/4HHuvsLd2929va2tbVRp6SKriMhgWd7J+hfAH9y9C8DMbgH+HPjhWCekcfAiOfFvV8LzD4/tOecvgzf9/X7fvvLKK1m4cCEf+chHALjqqqsoFAqsXr2abdu2US6XufrqqznnnHNGlGxPTw+XXHIJHR0dFAoFrr32Ws444wweeeQRLrroIvr6+ojjmJtvvpkjjjiCd7/73XR2dhJFEZ/73OdYvnz5QRUbsg3wzwAnm9k0YC/wOqAji4Q0F42IjNby5cv56Ec/Wgvwq1at4vbbb+eyyy5j5syZbNmyhZNPPpm3vvWtI3rw9Te/+U3MjIcffpjf/e53nHXWWTz++ONcf/31XH755Zx33nn09fURRRG/+MUvOOKII7j11lsB2LFjx5iULbMA7+5rzOwmYB1QAR4AVmSRloZJiuTEAVraWTnhhBPYvHkzzz77LF1dXRxyyCHMnz+fj33sY9x9990EQcCmTZt44YUXmD9//rDPe++993LppZcCsGTJEo488kgef/xxTjnlFL74xS/S2dnJ29/+do4++miWLVvGJz7xCT71qU/x5je/mdNOO21MypbpKBp3/1t3X+LuS939fe7em0U6usgqIgfjXe96FzfddBM33ngjy5cvZ+XKlXR1dbF27VrWr1/PvHnz6OnpGZO03vve9/Kzn/2MlpYWzj77bO68805e+tKXsm7dOpYtW8bf/M3f8IUvfGFM0srFbJLqgxeRg7F8+XI+9KEPsWXLFu666y5WrVrF3LlzKRaLrF69mqeffnrE5zzttNNYuXIlZ555Jo8//jjPPPMMxxxzDBs3bmTx4sVcdtllPPPMMzz00EMsWbKEQw89lPPPP5/Zs2dzww03jEm5chHg1QcvIgfj2GOPZdeuXbzoRS/i8MMP57zzzuMtb3kLy5Yto729nSVLloz4nB/+8Ie55JJLWLZsGYVCge9+97s0NTWxatUqfvCDH1AsFpk/fz6f+cxnuP/++7niiisIgoBisch11103JuWyiRQY29vbvaNj5Ndh33ndf9FUDFj5wZMzyJWIZOnRRx/lZS97WaOzMSkM9V2Z2Vp3bx/q+PxMNqYHfoiI7CMXXTSai0ZExtPDDz/M+973vn32NTU1sWbNmgblaGi5CPCBGZGusopMWu4+ojHmjbZs2TLWr18/rmmOpjs9H100gVrwIpNVc3MzW7du1UCJA3B3tm7dSnPzyGZcz00LXgFeZHJasGABnZ2djHaywamiubmZBQsWjOgzuQjwGgcvMnkVi0WOOuqoRmcjl/LRRaNx8CIig+QkwKsFLyIyUE4CvC6yiogMlIsArz54EZHBchHg1QcvIjJYTgK8hkmKiAyUowDf6FyIiEwsuQjwmotGRGSwzAK8mR1jZuvrXjvN7KNZpBWYofguIrKvLJ/J+hhwPICZhcAm4KdZpKVhkiIig41XF83rgCfdfeTPvRoGXWQVERlsvAL8e4AfD/WGmV1sZh1m1jHayYZMD/wQERkk8wBvZiXgrcC/DPW+u69w93Z3b29raxtVGhoHLyIy2Hi04N8ErHP3F7JKQMMkRUQGG48Afy776Z4ZK3rgh4jIYJkGeDObDrweuCXjdNSCFxEZINMHfrj7bmBOlmmA+uBFRIaSiztZNUxSRGSwHAX4RudCRGRiyUWA11w0IiKD5SLAay4aEZHBchLg1YIXERkoJwFeF1lFRAbKRYDXOHgRkcFyEeA1Dl5EZLCcBHi14EVEBspJgNdFVhGRgXIR4C0dJqluGhGRfrkI8IEZgMbCi4jUyUmAT5bqphER6ZePAJ9GeF1oFRHpl4sAb2rBi4gMkosArz54EZHBchLgk6Va8CIi/bJ+ZN9sM7vJzH5nZo+a2SlZpFNtwSvAi4j0y/SRfcDXgNvc/Z1mVgKmZZGImS6yiogMlFmAN7NZwOnAhQDu3gf0ZZFWtYtGNzqJiPTLsovmKKAL+I6ZPWBmN5jZ9IEHmdnFZtZhZh1dXV2jSihQC15EZJAsA3wBeCVwnbufAOwGrhx4kLuvcPd2d29va2sbVUK6yCoiMliWAb4T6HT3Nen2TSQBf8yZLrKKiAySWYB39+eB/zazY9JdrwN+m0VaGgcvIjJY1qNoLgVWpiNoNgIXZZHI4VvvY7HtVAteRKROpgHe3dcD7VmmAXBax2UsD19H7OdnnZSIyKSRiztZ3QJCYmINoxERqclJgA8JcPXBi4jUyUWAx4yAWH3wIiJ1chHg3UJCYiIFeBGRmlwFeE1VICLSLxcBHgswnChudEZERCaOXAT42igateBFRGpyEeCxkNBiIg2TFBGpyUWAdws0ikZEZIBcBHiCZBy8WvAiIv3yEeDVBy8iMkguAnxyJ2usUTQiInVyEeDVghcRGSwnAT7pg9dkYyIi/fIR4INkFI2mKhAR6ZePAF+di0YteBGRmkwf+GFmTwG7gAiouHs2D/+wgIAKFbXgRURqsn5kH8AZ7r4l0xSCkIA+jaIREamTky6agFA3OomI7GNYAd7MLjezmZb4lpmtM7OzhvFRB35pZmvN7OKDy+oBBCGBabpgEZF6w23B/5W77wTOAg4B3gf8/TA+d6q7vxJ4E/ARMzt94AFmdrGZdZhZR1dX13DzPeAkeuCHiMhAww3wli7PBn7g7o/U7dsvd9+ULjcDPwVOGuKYFe7e7u7tbW1tw8zOgMylk42pi0ZEpN9wA/xaM/slSYC/3cxagQNe0jSz6elxmNl0ktb/hoPJ7H4FoWaTFBEZYLijaD4AHA9sdPc9ZnYocNGf+Mw84KdmVk3nR+5+26hzegAWVMfBZ3F2EZHJabgB/hRgvbvvNrPzgVcCXzvQB9x9I3DcQeZveAJNVSAiMtBwu2iuA/aY2XHAJ4Ange9nlqsRMj3wQ0RkkOEG+IonYxDPAb7h7t8EWrPL1ggFGkUjIjLQcLtodpnZp0mGR55mZgFQzC5bI5QOk1QXjYhIv+G24JcDvSTj4Z8HFgDXZJarEbIwxHQnq4jIPoYV4NOgvhKYZWZvBnrcfUL1wSddNI3OiYjIxDHcqQreDfwGeBfwbmCNmb0zy4yNhAUhoamLRkSk3nD74D8LnJjekYqZtQH/DtyUVcZGwnSjk4jIIMPtgw+qwT21dQSfzV46Dl6jaERE+g23BX+bmd0O/DjdXg78IpssjVy1D15dNCIi/YYV4N39CjN7B/DqdNcKd/9pdtkaGQsL6WRjjc6JiMjEMewnOrn7zcDNGeZl1PpH0agFLyJSdcAAb2a7SB7aMegtwN19Zia5GiELCpqLRkRkgAMGeHefONMRHEh1Pni14EVEaibOSJiDEaQXWRXgRURq8hHgNReNiMggOQnwQToXTaMzIiIyceQjwFenKlAXjYhITeYB3sxCM3vAzH6eXSLpnaxqwouI1IxHC/5y4NFMUwhCAGKPMk1GRGQyyTTAm9kC4C+BG7JMh+TB3lisAC8iUpV1C/4fgU8C2fadWNqCV4AXEanJLMCnDwbZ7O5r/8RxF5tZh5l1dHV1jS6xtIvGFeBFRGqybMG/GnirmT0F/AQ408x+OPAgd1/h7u3u3t7W1ja6lKwa4HWRVUSkKrMA7+6fdvcF7r4IeA9wp7ufn0lilhRDLXgRkX65GQcP4BpFIyJSM+zpgg+Gu/8H8B+ZJZC24C2uZJaEiMhkk6sWfKw+eBGRmnwE+LQFjwK8iEhNTgJ80oJHffAiIjX5CPAaBy8iMkg+ArwpwIuIDJSPAJ+24DWKRkSkX64CvPrgRUT65STAJ8P5TaNoRERqchXgifsamw8RkQkkZwFeXTQiIlW5CvCui6wiIjW5CvB6opOISL9cBXjicmPzISIygeQswKsFLyJSlZMArxudREQGykeAD4vJ0hXgRUSq8hHgaxdZFeBFRKoyC/Bm1mxmvzGzB83sETP7fFZp1QK8pioQEanJ8pF9vcCZ7t5tZkXgXjP7N3f/9ZinVOuDV4AXEanKLMC7uwPd6WYxfXkmidVa8BomKSJSlWkfvJmFZrYe2Azc4e5rhjjmYjPrMLOOrq6u0SUUJBdZA7XgRURqMg3w7h65+/HAAuAkM1s6xDEr3L3d3dvb2tpGl1B1HLxHJD8cRERkXEbRuPt2YDXwxkwSSPvgC0TEiu8iIkC2o2jazGx2ut4CvB74XSaJpS34kIhypDnhRUQg21E0hwPfM7OQpCJZ5e4/zySlNMAXiYjUhBcRAbIdRfMQcEJW599HeidrSExFAV5EBMjLnazW3wdfUReNiAiQlwAfBMQEhKYuGhGRqnwEeMCDkIK6aEREavIT4K2QdtEowIuIQB4DfKw+eBERyFOAD0JCInXRiIik8hPgrZD0waddNM/v6OGWdZ0NzpWISONkeaPTuKq24KujaM674dc82bWbU19yGHNnNjc4dyIi4y83LXiCAkWLKMcxce9urtr+Wf7MHuO3z+1sdM5ERBoiNwHerb8Fv2PD7ZwWbuBzxR/w6HO7Gp01EZGGyE2AJyz298E/cQcAZQo89rxa8CIyNeUmwLsV0lE0Mbb9DwC81DrZvLOnwTkTEWmM3AR46u5ktT1/BGCW7aFn55YGZ0xEpDFyFOALFKhQiZxi7x/Z5S0AWPcLDc6YiEhj5CrAh8REUUSpbzuP+UIAWvq20FPWs1pFZOrJTYC3sEiBCO/bTcHLbAxeDMBctrGlu7fBuRMRGX+5CfAEBQoWEe7dCsCzpcUAzLXtdO1SgBeRqSfLZ7IuNLPVZvZbM3vEzC7PKi0ACk2UKBPsTS6w7mw+nKg4g7m2nc0K8CIyBWU5VUEF+IS7rzOzVmCtmd3h7r/NIjErNtNMGetJAnzUNBsP5zG3Ry14EZmaMmvBu/tz7r4uXd8FPAq8KKv0wlIzTfRhPTuSHc2zCGfOZ55tUwteRKakcemDN7NFJA/gXjPEexebWYeZdXR1dY06jaA4jSYrY73J1ARB8yysdT7zgx1qwYvIlJR5gDezGcDNwEfdfdC8Ae6+wt3b3b29ra1t1OkExWaa6SModwNQaGmFGfNpYxtdO/eO+rwiIpNVpgHezIokwX2lu9+SaVppH3zQt4vYjdK0mdA6j2Z66d65PcukRUQmpCxH0RjwLeBRd782q3RqCs00WRl6d9FNCzOaSzBjfvLeruczT15EZKLJsgX/auB9wJlmtj59nZ1ZaoVmQmLC3u3sooUZzQVonQdAuHcz7nqUn4hMLZkNk3T3ewHL6vyDFJOnNrX0baXbW2htKtRa8HPibWzfU+aQ6aVxy46ISKPl507WQhLgZ1S20U0L05v6W/BtGiopIlNQ7gL8rHgH3d7CjKYCNM8mDps0XYGITEm5C/CHsYNdtNDaXAAzounz0ukK9OAPEZla8hPg0z74wJxub2H2tGKy3TqfuagFLyJTT34CfNqCB9jODGZPSy6oBjPnMy+oC/A7Oqlc/1qe+cZb6Nre3YicioiMixwF+Kba6i5rZXopBMBa5zOvbkZJv+daCs8/wIu33M1N//xF4ljDJ0Ukn3IU4Ftqqz3F2ST3WQEz5tHKbrbv3AmVPuIHb+Tm6FSeDBdz6q5f8J9P6pmtIpJPOQrw/S34StMh/ftbk7Hw0c7nYdNawnI3d3ISR7zmIpYFT7H6v+4b75yKiIyL/AT4Yn8LPm6uC/DpzU7B7hfgD3cRERAfeSotx70dgNYnb9UzW0Ukl/IT4FsOra1u3N3fmq/e7DS9byt7HruTDfEiXvWyxTBrATvnHMfrWMM9T6ibRkTyJz8Bfvqc2urzlen9+9MW/FH2PKXn1nJffCyvPWZu8pHj384rgj/w67Xr9jnVlu5e+ipx9nkWEclQfgJ8na9ddEb/xrQ5xC2HcVnhFgpUeHLmSSw6LKkAwmPPAaD0xM/ZsbdMT1+Zlf/77/julz7C//iHm1n3zLZGZF9EZEzkK8B/eA385bUsXdjfmicICI7+C1qsjy6fySteXTeh5aFHsXfOUs7hLr7ysw42fPUcztv8Ff66+C/8oPLXfPmGH/KbP/xx/MshIjIGbCJNo9ve3u4dHR1jf+Kn7iW69ZM8uvhClpz1QQphXb224Ra46SLKHhIS88DLP8mfnfE2KiuXE+94ls/HH+ANp76Khbs3EG1+jG0zXsLhp/8VCxYeOfb5FBEZITNb6+7tQ743JQL8gbgT/efX2fbInew+/q848lVJtw27t9L3o/dS2vTr2qEv+Gzm2XbKHvJI8wnMmDWHlt4uCn076S61YYcexcyZsynGeyEqE04/lJbZcwlL0yAIISgmo32KLcmdt8WWZHhnoTl5LwghLEJQSN+flmzb+M26LCKTiwL8aFX6KD/2SzZu3Uv3YSewcMEC4q7H2LT6nzls02o8KrOZ2ey26czljyzkBZqosJcSFUJmsZuiHdwQTLcAD4p4UEgCfxBiQQELQqhbYmFaiYTpeiHdLoAFdevV94IB22F6XGHAsUOdc8B2/XED0xgyL4UB5xgiL/uct7CfNKrrqgBl6lKAz4C7s7OnQnMxoKkQUo5inuzq5vebu4nS6Q/29FbYvm0L23fuoq/cR6XcS7m3h3LPbqLevXh5D1bpgaiXuFKhSERoEUUimijTTC8t1pfsJ0qeWEVEIV2GFtMUOKXAKVpcWxbTZXJ8TCFdLyR3ARASp8vknObJunlEQETgcbLuyT5Lt80n6v0Ctm8lEoQQlpJfQWEp/VU04BdS/SssQVhdpsfX1kvJr6zqvmolhSUVS33luc85i8PfDovJr7XitCQtVVgyAgcK8Jk90cnMvg28Gdjs7kuzSqdRzIxZLcXadjEMWDJ/Jkvmzxxw5PD66uPY6a3E7C1HyasvoqdufW853U7X95YjeurW9/Tt+35POaYSx/RVYsqRp8vqK9nui0Y6FNQJ8FpF0b/srzQKRAQW0xw4TaHTFDhNITQFMaUgXYZJpdQUxBQNSkFSOZUsphg6RdJKKnCKRBQDp2AxRUvSLgROIa24Cum+sFaJxYRUKHiFMO6jEPcRUCHwmNDLmEeEXsEqZSzem1RiUR/ElWQZlSEuQ9QHlb5kGZdH+D0dBAuSQG9hEuiLLf3Bv2lGUmmV98CshVDpSe7UjspQmt5fQVQrt0IJwqa0G7ApXa+r+Krdg7X1pv7jVcnkQmYBHvgu8A3g+xmmkRtBYLSUQlrSSdLGg7tTiT0J+hWnN4ooR045rQz6ov4KopJuD1wvR3G6PXi9XImpxJ4cW61gYmdnul6J6yse36cC2uc8UUwl8lFUSMNnllTSpTCgEFptvVgyigG0hM60oEJT6BRDKAVQCo1i4JQCaA5iSkGcVmROUxBRtJgmS/aXLPnlVUr3F0iWRSrJy/soRnspxnspxj2EQECFYtRDEO0lrOwh6NtFsHc7hmP/vSYJxH+4J1mW90J5N/gYfUfViqbQDDjEFSi1JhVEffdY9RdQ7VdOWon07U72V685hSXAoDQtqZCicpKGBf2fLbQk035X98dR8l5QSPJQak2W7oO78QZ1J6bLsJTkHaDlkOT7MUvyYkF/RVZbr9tvQVKeYjP07Ey/k5akYq30QuvhyfkqPVCakXQ1VvrS/JKkW72GFqfpmiXlqk8bkjJlUKlm+UzWu81sUVbnl4NnZhTTYEYJoPinPtJQ+1RIQwT/akVVjqsVyoD1aOjKZOj1/b+3t5JWgr116VXTHnB8JaPZSqv/bsUwoBQnlVFpekBTEDO9EDHNKjRZhSbKycsqlChTsmS7RIUi1fUyxerSk1fBIkpxLyV6wULcAprjPRS8TOBOUEm7DT0i9DIF7yb0crIe91EJmwk8ohD3Uoh7COIyhlOo7AUg8ApuAXFQpFDZAxjGZL650ACvW09VpzH3NKi7J8cVmvuvR4Xp4IvLHhjzXGXZgh8WM7sYuBjgxS9+cYNzIxPZPhXSJOHuw65E+ir16/v+ghq8r25Z99n6fT0Ou2MncsfdiWIncvrXY8cdInfi2Ind03WS9XRf7CTrQx3jyTlG8c2kS6NABQMCYqbTQ4WAAjEVwrQiqhBjTLce4jR4Vq8v9V9PimvXqEKcIO2ya6IPgABnGj1EhLXtapej4QTmFCwJzWbJ/tBimqgwjV56rQkHilahh2bcAub6H6lYkZiAafTgGLEFaX5DAnNavJeYAMMJialYgZCYUlQGM8JKRMEidsfzeNfB/akNqeEB3t1XACsgucja4OyIjCkzo1QwSoXJUymNlNdXAvUVQ5xWHmkFEqXHxXF95eFE+6tQ6j9XV6EkFQ0DzjEgzeox9ZXXkBVW/zHVyspx0v+IY6cbavt7vb/Mnek+T49NKrpkO/Z991fPGbvX7UvO48DM5kI+A7yITG5mRmgQBrowO9Hkt1khIjLFZRbgzezHwH3AMWbWaWYfyCotEREZLMtRNOdmdW4REfnT1EUjIpJTCvAiIjmlAC8iklMK8CIiOaUALyKSUxNqumAz6wKeHuXHDwO2jGF2JgOVeWpQmaeG0Zb5SHdvG+qNCRXgD4aZdexvTuS8UpmnBpV5asiizOqiERHJKQV4EZGcylOAX9HoDDSAyjw1qMxTw5iXOTd98CIisq88teBFRKSOAryISE5N+gBvZm80s8fM7PdmdmWj8zNWzOzbZrbZzDbU7TvUzO4wsyfS5SHpfjOzr6ffwUNm9srG5Xz0zGyhma02s9+a2SNmdnm6P7flNrNmM/uNmT2Ylvnz6f6jzGxNWrYbzayU7m9Kt3+fvr+okfk/GGYWmtkDZvbzdDvXZTazp8zsYTNbb2Yd6b5M/7YndYA3sxD4JvAm4OXAuWb28sbmasx8F3jjgH1XAr9y96OBX6XbkJT/6PR1MXDdOOVxrFWAT7j7y4GTgY+k/555LncvcKa7HwccD7zRzE4G/gH4qru/BNgGVJ+n8AFgW7r/q+lxk9XlwKN121OhzGe4+/F1492z/dv29IG8k/EFnALcXrf9aeDTjc7XGJZvEbChbvsx4PB0/XDgsXT9/wDnDnXcZH4B/xd4/VQpNzANWAe8iuSOxkK6v/Z3DtwOnJKuF9LjrNF5H0VZF6QB7Uzg5yTPu857mZ8CDhuwL9O/7UndggdeBPx33XZnui+v5rn7c+n688C8dD1330P6M/wEYA05L3faVbEe2AzcATwJbHf3SnpIfblqZU7f3wHMGd8cj4l/BD4JxOn2HPJfZgd+aWZrzezidF+mf9t66PYk5e5uZrkc42pmM4CbgY+6+06z/oc557Hc7h4Bx5vZbOCnwJIGZylTZvZmYLO7rzWz1zY6P+PoVHffZGZzgTvM7Hf1b2bxtz3ZW/CbgIV12wvSfXn1gpkdDpAuN6f7c/M9mFmRJLivdPdb0t25LzeAu28HVpN0T8w2s2oDrL5ctTKn788Cto5zVg/Wq4G3mtlTwE9Iumm+Rr7LjLtvSpebSSryk8j4b3uyB/j7gaPTq+8l4D3Azxqcpyz9DLggXb+ApI+6uv/96ZX3k4EddT/7Jg1LmurfAh5192vr3sptuc2sLW25Y2YtJNccHiUJ9O9MDxtY5up38U7gTk87aScLd/+0uy9w90Uk/8/e6e7nkeMym9l0M2utrgNnARvI+nF4HCcAAAJySURBVG+70RcexuDCxdnA4yT9lp9tdH7GsFw/Bp4DyiT9bx8g6Xf8FfAE8O/AoemxRjKa6EngYaC90fkfZZlPJemnfAhYn77OznO5gVcAD6Rl3gD8r3T/YuA3wO+BfwGa0v3N6fbv0/cXN7oMB1n+1wI/z3uZ07I9mL4eqcaqrP+2NVWBiEhOTfYuGhER2Q8FeBGRnFKAFxHJKQV4EZGcUoAXEckpBXjJPTOL0hn8qq8xm3XUzBZZ3YyfIhOJpiqQqWCvux/f6EyIjDe14GXKSufn/nI6R/dvzOwl6f5FZnZnOg/3r8zsxen+eWb203Tu9gfN7M/TU4Vm9s/pfO6/TO9Ixcwus2Ru+4fM7CcNKqZMYQrwMhW0DOiiWV733g53XwZ8g2SGQ4B/Ar7n7q8AVgJfT/d/HbjLk7nbX0lyRyIkc3Z/092PBbYD70j3XwmckJ7nf2ZVOJH90Z2skntm1u3uM4bY/xTJwzY2ppOcPe/uc8xsC8nc2+V0/3PufpiZdQEL3L237hyLgDs8eWADZvYpoOjuV5vZbUA38K/Av7p7d8ZFFdmHWvAy1fl+1keit249ov/a1l+SzCfySuD+upkSRcaFArxMdcvrlvel6/9FMsshwHnAPen6r4BLoPaQjln7O6mZBcBCd18NfIpkittBvyJEsqQWhUwFLekTk6puc/fqUMlDzOwhklb4uem+S4HvmNkVQBdwUbr/cmCFmX2ApKV+CcmMn0MJgR+mlYABX/dkvneRcaM+eJmy0j74dnff0ui8iGRBXTQiIjmlFryISE6pBS8iklMK8CIiOaUALyKSUwrwIiI5pQAvIpJT/x9p+JlT/v63jAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.4316504927229007\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMCADSzG0rNs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "803c820c-4264-41df-a391-3465e95673a1"
      },
      "source": [
        "prednn=m.predict(X_testB)\n",
        "print(len(prednn))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "647\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bk6S9xG3TXyD",
        "colab_type": "text"
      },
      "source": [
        "**Linear Regression**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVAqAratTcmp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "lrmodel=LinearRegression()\n",
        "lrmodel.fit(X_trainB,y_trainB)\n",
        "predictlr = lrmodel.predict(X_testB) # training linear regressor\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWO6v1H7iDBC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "79ce2cf8-580b-444b-8efa-a1279f8c2dea"
      },
      "source": [
        "print(sklearn.metrics.r2_score(y_testB,lrmodel.predict(X_testB)))\n",
        "print(sklearn.metrics.r2_score(y_trainB,lrmodel.predict(X_trainB)))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4236345416239852\n",
            "0.45801690528243477\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OE8wz31hQfcr",
        "colab_type": "text"
      },
      "source": [
        "**Support Vector Regressor**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLysGV00QrwR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import SVR\n",
        "svrmodel=SVR()\n",
        "svrmodel.fit(X_trainB,y_trainB)\n",
        "predictsvr = svrmodel.predict(X_testB) # training support vector regressor"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hf0fgwzgh2Dv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "f59c8902-128f-49eb-acf5-07adc3827fd2"
      },
      "source": [
        "print(sklearn.metrics.r2_score(y_testB,svrmodel.predict(X_testB)))\n",
        "print(sklearn.metrics.r2_score(y_trainB,svrmodel.predict(X_trainB)))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.42518953528361747\n",
            "0.4997280135636202\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgAZPKz2TrM4",
        "colab_type": "text"
      },
      "source": [
        "KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XxoAgunUmAP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "knnmodel=KNeighborsRegressor()\n",
        "knnmodel.fit(X_trainB,y_trainB)\n",
        "predictknn = knnmodel.predict(X_testB) # training KNN regressor"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BMgJu3rehcvb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "8e77ea2f-bd31-42ce-afda-fad2bd8f0828"
      },
      "source": [
        "print(sklearn.metrics.r2_score(y_testB,knnmodel.predict(X_testB))) \n",
        "print(sklearn.metrics.r2_score(y_trainB,knnmodel.predict(X_trainB))) \n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.46152692985267907\n",
            "0.6359005911710629\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTEJmvlUXBqX",
        "colab_type": "text"
      },
      "source": [
        "ADABOOST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7Vg5Xh0XFpK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "adbmodel=AdaBoostRegressor()\n",
        "adbmodel.fit(X_trainB,y_trainB)\n",
        "predictadb = adbmodel.predict(X_testB) # training AdaBoost regressor"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLhLXnn1hMwS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "b24d38e6-26f8-4204-e124-92aa17fcac46"
      },
      "source": [
        "print(sklearn.metrics.r2_score(y_testB,adbmodel.predict(X_testB)))\n",
        "print(sklearn.metrics.r2_score(y_trainB,adbmodel.predict(X_trainB)))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.39947878366619105\n",
            "0.4064553828265012\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AINwkX5Fmosp",
        "colab_type": "text"
      },
      "source": [
        "**Randmom Forest Regressor**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDGY0Ne8e9u_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RFmodel = RandomForestRegressor(n_estimators=900) \n",
        "RFmodel.fit(X_trainB,y_trainB)\n",
        "predictrf = RFmodel.predict(X_testB) # training random forest regressor\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAeiPiE8g35w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "f838068c-9d5d-44f8-d319-4ad67409c3ca"
      },
      "source": [
        "print(sklearn.metrics.r2_score(y_testB,RFmodel.predict(X_testB))) \n",
        "print(sklearn.metrics.r2_score(y_trainB,RFmodel.predict(X_trainB))) "
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6707859982086705\n",
            "0.9242618480386157\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aD1QDRSENyFX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "baebe08d-8d7f-48ea-de0a-ecdddc215e0f"
      },
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "# Number of trees in random forest\n",
        "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
        "# Number of features to consider at every split\n",
        "max_features = ['auto', 'sqrt']\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
        "max_depth.append(None)\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [2, 5, 10]\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [1, 2, 4]\n",
        "# Method of selecting samples for training each tree\n",
        "bootstrap = [True, False]\n",
        "# Create the random grid\n",
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf,\n",
        "               'bootstrap': bootstrap}\n",
        "print(random_grid)\n",
        "{'bootstrap': [True, False],\n",
        " 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
        " 'max_features': ['auto', 'sqrt'],\n",
        " 'min_samples_leaf': [1, 2, 4],\n",
        " 'min_samples_split': [2, 5, 10],\n",
        " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bootstrap': [True, False],\n",
              " 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
              " 'max_features': ['auto', 'sqrt'],\n",
              " 'min_samples_leaf': [1, 2, 4],\n",
              " 'min_samples_split': [2, 5, 10],\n",
              " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bohnt86NN-uf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "f193bdb5-f7ee-4384-a087-1371df4cca00"
      },
      "source": [
        "# Use the random grid to search for best hyperparameters\n",
        "# First create the base model to tune\n",
        "rf = RandomForestRegressor()\n",
        "# Random search of parameters, using 3 fold cross validation, \n",
        "# search across 100 different combinations, and use all available cores\n",
        "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
        "# Fit the random search model\n",
        "rf_random.fit(X_trainB,y_trainB) # making random forest regressor\n",
        "predictrfrand = rf_random.predict(X_testB) "
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  1.6min\n",
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "[Parallel(n_jobs=-1)]: Done 158 tasks      | elapsed:  6.7min\n",
            "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed: 12.5min finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kwUC3IQpUp5c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.externals import joblib\n",
        "joblib.dump(rf_random, \"rf_B_optimized.pkl\") \n",
        "my_model_loaded = joblib.load(\"rf_B_optimized.pkl\")"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Chd9JrdHYn7C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "7b51dcbe-6802-4b63-d03e-db94de2dbda5"
      },
      "source": [
        "print(sklearn.metrics.r2_score(y_testB,   rf_random.predict(X_testB)))\n",
        "print(sklearn.metrics.r2_score(y_trainB,  rf_random.predict(X_trainB)))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6895596532180643\n",
            "0.926957116926998\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqKqx5jNXWzG",
        "colab_type": "text"
      },
      "source": [
        "Gradient Boosting Regressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ko_gIv0XeG6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "gbrmodel=GradientBoostingRegressor()\n",
        "gbrmodel.fit(X_trainB,y_trainB)\n",
        "predictgbr = gbrmodel.predict(X_testB) # training Gradient Boosting regressor"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNiNADVsgOg4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "237faa50-278e-4fca-b9e1-37c2888a3c67"
      },
      "source": [
        "print(sklearn.metrics.r2_score(y_testB,gbrmodel.predict(X_testB)))\n",
        "print(sklearn.metrics.r2_score(y_trainB,gbrmodel.predict(X_trainB)))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4671152062249727\n",
            "0.5384180409491339\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDhQwI9kbIlP",
        "colab_type": "text"
      },
      "source": [
        "#### ENSEMBLE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFUwm1z1ZmqM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "54d49e60-9237-44a7-ded9-aea6b9be1daf"
      },
      "source": [
        "df1=pd.DataFrame()\n",
        "df1['f1']=predictlr\n",
        "df1['f2']=predictgbr\n",
        "df1['f3']=predictsvr\n",
        "df1['f4']=predictrfrand\n",
        "df1['f5']=prednn\n",
        "df1['l']=y_testB\n",
        "df1.head()"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f1</th>\n",
              "      <th>f2</th>\n",
              "      <th>f3</th>\n",
              "      <th>f4</th>\n",
              "      <th>f5</th>\n",
              "      <th>l</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.671470</td>\n",
              "      <td>5.788697</td>\n",
              "      <td>5.688830</td>\n",
              "      <td>4.917058</td>\n",
              "      <td>5.424296</td>\n",
              "      <td>3.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6.759800</td>\n",
              "      <td>6.821102</td>\n",
              "      <td>6.591826</td>\n",
              "      <td>7.021700</td>\n",
              "      <td>6.785369</td>\n",
              "      <td>6.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>7.301531</td>\n",
              "      <td>7.350622</td>\n",
              "      <td>7.397771</td>\n",
              "      <td>8.848017</td>\n",
              "      <td>7.288290</td>\n",
              "      <td>9.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7.113331</td>\n",
              "      <td>7.268155</td>\n",
              "      <td>6.864422</td>\n",
              "      <td>7.289564</td>\n",
              "      <td>7.322757</td>\n",
              "      <td>6.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8.038928</td>\n",
              "      <td>8.282809</td>\n",
              "      <td>8.238469</td>\n",
              "      <td>8.046467</td>\n",
              "      <td>8.018075</td>\n",
              "      <td>7.2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         f1        f2        f3        f4        f5    l\n",
              "0  5.671470  5.788697  5.688830  4.917058  5.424296  3.7\n",
              "1  6.759800  6.821102  6.591826  7.021700  6.785369  6.7\n",
              "2  7.301531  7.350622  7.397771  8.848017  7.288290  9.5\n",
              "3  7.113331  7.268155  6.864422  7.289564  7.322757  6.1\n",
              "4  8.038928  8.282809  8.238469  8.046467  8.018075  7.2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJN66RsFaPld",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fa08faa6-1c41-4b7b-9c87-e91217f9aa5e"
      },
      "source": [
        "x=df1.iloc[:,:-1]\n",
        "y=df1.iloc[:,-1:]\n",
        "y.head()\n",
        "x1, x2, y1, y2= train_test_split(x,y,test_size=0.4,random_state=101)\n",
        "from sklearn.linear_model import LinearRegression\n",
        "lrmodel1=LinearRegression()\n",
        "lrmodel1.fit(x1,y1)\n",
        "sklearn.metrics.r2_score(y2,lrmodel1.predict(x2))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6969954501540173"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyclYoQrSIie",
        "colab_type": "text"
      },
      "source": [
        "## Model for sem1A batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iSnXhv67QXRM",
        "colab": {}
      },
      "source": [
        "## now we have to do sem 1 b batch separately because of difference in subjects ##\n",
        "X=vp[['Math1', 'Physics1', 'Chem', 'BME', 'Workshop', 'FEC1']] # taking the semester 1 subjects for A batch from our dataset\n",
        "#Y = Final[['Math2', 'Physics2', 'Chem', 'BME', 'Workshop', 'FEC2']]\n",
        "\n",
        "Y=vp['CGA2']\n",
        "# converting everything to arrays and splitting our data into train and test sets\n",
        "X_train2, X_test2, y_train2, y_test2 = train_test_split(X,Y,test_size=0.3,random_state=101)\n",
        "X_train2 = np.array(X_train2)\n",
        "X_test2 = np.array(X_test2)\n",
        "y_train2 = np.array(y_train2)\n",
        "y_test2 = np.array(y_test2)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioH4yNykooEE",
        "colab_type": "text"
      },
      "source": [
        "**Neural Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2qJhn9a3QvQW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a9feb8ba-78fd-453e-8ed7-ae9a56638f77"
      },
      "source": [
        "m,h,r = NNpredict(X_train2,X_test2,y_train2,y_test2) # training neural network\n",
        "plot_graphs(h,'loss') #plotting loss graph\n",
        "print(r) # prining r2 score"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "1/1 [==============================] - 0s 102ms/step - loss: 48.8693 - val_loss: 42.6971\n",
            "Epoch 2/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 42.1506 - val_loss: 36.1680\n",
            "Epoch 3/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 35.7118 - val_loss: 30.2317\n",
            "Epoch 4/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 29.8338 - val_loss: 25.0946\n",
            "Epoch 5/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 24.7316 - val_loss: 20.6122\n",
            "Epoch 6/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 20.2885 - val_loss: 16.6270\n",
            "Epoch 7/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 16.3514 - val_loss: 13.0957\n",
            "Epoch 8/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 12.8692 - val_loss: 10.0176\n",
            "Epoch 9/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 9.8382 - val_loss: 7.3981\n",
            "Epoch 10/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 7.2599 - val_loss: 5.2412\n",
            "Epoch 11/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 5.1399 - val_loss: 3.5491\n",
            "Epoch 12/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 3.4787 - val_loss: 2.3143\n",
            "Epoch 13/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 2.2673 - val_loss: 1.5011\n",
            "Epoch 14/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.4714 - val_loss: 1.0472\n",
            "Epoch 15/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.0296 - val_loss: 0.8831\n",
            "Epoch 16/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.8729 - val_loss: 0.9461\n",
            "Epoch 17/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.9373 - val_loss: 1.1680\n",
            "Epoch 18/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.1567 - val_loss: 1.4789\n",
            "Epoch 19/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.4632 - val_loss: 1.8105\n",
            "Epoch 20/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.7899 - val_loss: 2.1053\n",
            "Epoch 21/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 2.0805 - val_loss: 2.3230\n",
            "Epoch 22/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 2.2954 - val_loss: 2.4435\n",
            "Epoch 23/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.4141 - val_loss: 2.4627\n",
            "Epoch 24/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.4329 - val_loss: 2.3905\n",
            "Epoch 25/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 2.3616 - val_loss: 2.2459\n",
            "Epoch 26/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 2.2186 - val_loss: 2.0517\n",
            "Epoch 27/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 2.0266 - val_loss: 1.8316\n",
            "Epoch 28/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.8086 - val_loss: 1.6065\n",
            "Epoch 29/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.5858 - val_loss: 1.3944\n",
            "Epoch 30/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.3757 - val_loss: 1.2085\n",
            "Epoch 31/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.1917 - val_loss: 1.0575\n",
            "Epoch 32/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.0424 - val_loss: 0.9452\n",
            "Epoch 33/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.9315 - val_loss: 0.8715\n",
            "Epoch 34/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.8587 - val_loss: 0.8330\n",
            "Epoch 35/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.8205 - val_loss: 0.8236\n",
            "Epoch 36/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.8115 - val_loss: 0.8364\n",
            "Epoch 37/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.8243 - val_loss: 0.8639\n",
            "Epoch 38/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.8517 - val_loss: 0.8989\n",
            "Epoch 39/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.8862 - val_loss: 0.9348\n",
            "Epoch 40/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.9218 - val_loss: 0.9668\n",
            "Epoch 41/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9533 - val_loss: 0.9912\n",
            "Epoch 42/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9773 - val_loss: 1.0059\n",
            "Epoch 43/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.9917 - val_loss: 1.0103\n",
            "Epoch 44/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.9959 - val_loss: 1.0047\n",
            "Epoch 45/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9902 - val_loss: 0.9904\n",
            "Epoch 46/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9759 - val_loss: 0.9692\n",
            "Epoch 47/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.9549 - val_loss: 0.9434\n",
            "Epoch 48/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.9293 - val_loss: 0.9155\n",
            "Epoch 49/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.9017 - val_loss: 0.8876\n",
            "Epoch 50/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.8741 - val_loss: 0.8619\n",
            "Epoch 51/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.8486 - val_loss: 0.8397\n",
            "Epoch 52/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.8267 - val_loss: 0.8222\n",
            "Epoch 53/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.8094 - val_loss: 0.8098\n",
            "Epoch 54/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.7972 - val_loss: 0.8024\n",
            "Epoch 55/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.7899 - val_loss: 0.7995\n",
            "Epoch 56/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.7869 - val_loss: 0.8002\n",
            "Epoch 57/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.7874 - val_loss: 0.8033\n",
            "Epoch 58/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.7902 - val_loss: 0.8077\n",
            "Epoch 59/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.7940 - val_loss: 0.8122\n",
            "Epoch 60/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.7977 - val_loss: 0.8157\n",
            "Epoch 61/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.8003 - val_loss: 0.8173\n",
            "Epoch 62/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.8008 - val_loss: 0.8165\n",
            "Epoch 63/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.7990 - val_loss: 0.8129\n",
            "Epoch 64/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.7947 - val_loss: 0.8071\n",
            "Epoch 65/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.7882 - val_loss: 0.8004\n",
            "Epoch 66/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.7810 - val_loss: 0.7938\n",
            "Epoch 67/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.7739 - val_loss: 0.7880\n",
            "Epoch 68/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.7677 - val_loss: 0.7834\n",
            "Epoch 69/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.7628 - val_loss: 0.7805\n",
            "Epoch 70/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.7595 - val_loss: 0.7791\n",
            "Epoch 71/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.7576 - val_loss: 0.7788\n",
            "Epoch 72/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.7567 - val_loss: 0.7790\n",
            "Epoch 73/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.7561 - val_loss: 0.7789\n",
            "Epoch 74/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.7552 - val_loss: 0.7781\n",
            "Epoch 75/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.7535 - val_loss: 0.7764\n",
            "Epoch 76/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.7508 - val_loss: 0.7738\n",
            "Epoch 77/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.7472 - val_loss: 0.7714\n",
            "Epoch 78/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.7437 - val_loss: 0.7693\n",
            "Epoch 79/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.7405 - val_loss: 0.7679\n",
            "Epoch 80/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.7381 - val_loss: 0.7676\n",
            "Epoch 81/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.7368 - val_loss: 0.7680\n",
            "Epoch 82/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.7363 - val_loss: 0.7687\n",
            "Epoch 83/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.7361 - val_loss: 0.7688\n",
            "Epoch 84/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.7355 - val_loss: 0.7682\n",
            "Epoch 85/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.7343 - val_loss: 0.7668\n",
            "Epoch 86/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.7325 - val_loss: 0.7648\n",
            "Epoch 87/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.7304 - val_loss: 0.7629\n",
            "Epoch 88/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.7284 - val_loss: 0.7613\n",
            "Epoch 89/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.7268 - val_loss: 0.7604\n",
            "Epoch 90/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.7258 - val_loss: 0.7599\n",
            "Epoch 91/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.7252 - val_loss: 0.7596\n",
            "Epoch 92/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.7247 - val_loss: 0.7592\n",
            "Epoch 93/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.7240 - val_loss: 0.7585\n",
            "Epoch 94/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.7230 - val_loss: 0.7576\n",
            "Epoch 95/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.7217 - val_loss: 0.7568\n",
            "Epoch 96/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.7204 - val_loss: 0.7562\n",
            "Epoch 97/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.7194 - val_loss: 0.7559\n",
            "Epoch 98/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.7186 - val_loss: 0.7558\n",
            "Epoch 99/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.7180 - val_loss: 0.7557\n",
            "Epoch 100/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.7176 - val_loss: 0.7553\n",
            "Epoch 101/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.7170 - val_loss: 0.7546\n",
            "Epoch 102/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.7162 - val_loss: 0.7536\n",
            "Epoch 103/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.7153 - val_loss: 0.7526\n",
            "Epoch 104/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.7145 - val_loss: 0.7518\n",
            "Epoch 105/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.7138 - val_loss: 0.7511\n",
            "Epoch 106/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.7132 - val_loss: 0.7505\n",
            "Epoch 107/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.7127 - val_loss: 0.7500\n",
            "Epoch 108/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.7122 - val_loss: 0.7493\n",
            "Epoch 109/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.7116 - val_loss: 0.7487\n",
            "Epoch 110/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.7109 - val_loss: 0.7481\n",
            "Epoch 111/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.7103 - val_loss: 0.7476\n",
            "Epoch 112/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.7097 - val_loss: 0.7471\n",
            "Epoch 113/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.7092 - val_loss: 0.7467\n",
            "Epoch 114/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.7088 - val_loss: 0.7462\n",
            "Epoch 115/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.7083 - val_loss: 0.7455\n",
            "Epoch 116/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.7078 - val_loss: 0.7447\n",
            "Epoch 117/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.7072 - val_loss: 0.7439\n",
            "Epoch 118/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.7066 - val_loss: 0.7432\n",
            "Epoch 119/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.7061 - val_loss: 0.7425\n",
            "Epoch 120/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.7057 - val_loss: 0.7418\n",
            "Epoch 121/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.7052 - val_loss: 0.7411\n",
            "Epoch 122/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.7048 - val_loss: 0.7405\n",
            "Epoch 123/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.7043 - val_loss: 0.7399\n",
            "Epoch 124/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.7038 - val_loss: 0.7394\n",
            "Epoch 125/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.7033 - val_loss: 0.7389\n",
            "Epoch 126/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.7029 - val_loss: 0.7383\n",
            "Epoch 127/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.7025 - val_loss: 0.7378\n",
            "Epoch 128/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.7021 - val_loss: 0.7372\n",
            "Epoch 129/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.7017 - val_loss: 0.7365\n",
            "Epoch 130/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.7013 - val_loss: 0.7359\n",
            "Epoch 131/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.7009 - val_loss: 0.7353\n",
            "Epoch 132/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.7005 - val_loss: 0.7347\n",
            "Epoch 133/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.7001 - val_loss: 0.7341\n",
            "Epoch 134/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6998 - val_loss: 0.7336\n",
            "Epoch 135/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6994 - val_loss: 0.7331\n",
            "Epoch 136/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6990 - val_loss: 0.7326\n",
            "Epoch 137/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6987 - val_loss: 0.7321\n",
            "Epoch 138/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6983 - val_loss: 0.7316\n",
            "Epoch 139/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6980 - val_loss: 0.7311\n",
            "Epoch 140/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6977 - val_loss: 0.7306\n",
            "Epoch 141/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6974 - val_loss: 0.7302\n",
            "Epoch 142/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6971 - val_loss: 0.7297\n",
            "Epoch 143/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6968 - val_loss: 0.7292\n",
            "Epoch 144/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6965 - val_loss: 0.7287\n",
            "Epoch 145/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6963 - val_loss: 0.7283\n",
            "Epoch 146/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6960 - val_loss: 0.7278\n",
            "Epoch 147/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6957 - val_loss: 0.7274\n",
            "Epoch 148/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6954 - val_loss: 0.7270\n",
            "Epoch 149/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6951 - val_loss: 0.7266\n",
            "Epoch 150/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6948 - val_loss: 0.7262\n",
            "Epoch 151/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6945 - val_loss: 0.7258\n",
            "Epoch 152/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6943 - val_loss: 0.7253\n",
            "Epoch 153/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6940 - val_loss: 0.7249\n",
            "Epoch 154/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.6937 - val_loss: 0.7245\n",
            "Epoch 155/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6935 - val_loss: 0.7241\n",
            "Epoch 156/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.6932 - val_loss: 0.7237\n",
            "Epoch 157/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6930 - val_loss: 0.7233\n",
            "Epoch 158/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6927 - val_loss: 0.7229\n",
            "Epoch 159/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6925 - val_loss: 0.7226\n",
            "Epoch 160/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6922 - val_loss: 0.7222\n",
            "Epoch 161/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6920 - val_loss: 0.7218\n",
            "Epoch 162/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6918 - val_loss: 0.7215\n",
            "Epoch 163/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6915 - val_loss: 0.7211\n",
            "Epoch 164/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6913 - val_loss: 0.7207\n",
            "Epoch 165/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.6911 - val_loss: 0.7204\n",
            "Epoch 166/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6909 - val_loss: 0.7200\n",
            "Epoch 167/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6907 - val_loss: 0.7197\n",
            "Epoch 168/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6905 - val_loss: 0.7194\n",
            "Epoch 169/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6902 - val_loss: 0.7191\n",
            "Epoch 170/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6900 - val_loss: 0.7188\n",
            "Epoch 171/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6898 - val_loss: 0.7184\n",
            "Epoch 172/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6896 - val_loss: 0.7181\n",
            "Epoch 173/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6894 - val_loss: 0.7179\n",
            "Epoch 174/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6892 - val_loss: 0.7176\n",
            "Epoch 175/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.6890 - val_loss: 0.7173\n",
            "Epoch 176/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.6888 - val_loss: 0.7170\n",
            "Epoch 177/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6886 - val_loss: 0.7167\n",
            "Epoch 178/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.6884 - val_loss: 0.7163\n",
            "Epoch 179/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6882 - val_loss: 0.7160\n",
            "Epoch 180/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6880 - val_loss: 0.7156\n",
            "Epoch 181/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6878 - val_loss: 0.7153\n",
            "Epoch 182/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6876 - val_loss: 0.7150\n",
            "Epoch 183/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6874 - val_loss: 0.7147\n",
            "Epoch 184/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6872 - val_loss: 0.7144\n",
            "Epoch 185/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6870 - val_loss: 0.7141\n",
            "Epoch 186/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.6868 - val_loss: 0.7138\n",
            "Epoch 187/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6866 - val_loss: 0.7135\n",
            "Epoch 188/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6864 - val_loss: 0.7132\n",
            "Epoch 189/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.6862 - val_loss: 0.7130\n",
            "Epoch 190/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6861 - val_loss: 0.7127\n",
            "Epoch 191/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.6859 - val_loss: 0.7125\n",
            "Epoch 192/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6857 - val_loss: 0.7122\n",
            "Epoch 193/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6855 - val_loss: 0.7119\n",
            "Epoch 194/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6853 - val_loss: 0.7116\n",
            "Epoch 195/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6851 - val_loss: 0.7113\n",
            "Epoch 196/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6849 - val_loss: 0.7110\n",
            "Epoch 197/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6847 - val_loss: 0.7107\n",
            "Epoch 198/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6845 - val_loss: 0.7104\n",
            "Epoch 199/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6843 - val_loss: 0.7102\n",
            "Epoch 200/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6842 - val_loss: 0.7100\n",
            "Epoch 201/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6840 - val_loss: 0.7097\n",
            "Epoch 202/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6838 - val_loss: 0.7096\n",
            "Epoch 203/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6836 - val_loss: 0.7094\n",
            "Epoch 204/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6835 - val_loss: 0.7092\n",
            "Epoch 205/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6833 - val_loss: 0.7090\n",
            "Epoch 206/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6831 - val_loss: 0.7088\n",
            "Epoch 207/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6830 - val_loss: 0.7087\n",
            "Epoch 208/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6828 - val_loss: 0.7086\n",
            "Epoch 209/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6826 - val_loss: 0.7085\n",
            "Epoch 210/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.6825 - val_loss: 0.7083\n",
            "Epoch 211/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6823 - val_loss: 0.7082\n",
            "Epoch 212/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6821 - val_loss: 0.7080\n",
            "Epoch 213/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6820 - val_loss: 0.7079\n",
            "Epoch 214/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6818 - val_loss: 0.7078\n",
            "Epoch 215/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.6816 - val_loss: 0.7076\n",
            "Epoch 216/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6815 - val_loss: 0.7075\n",
            "Epoch 217/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6813 - val_loss: 0.7073\n",
            "Epoch 218/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6812 - val_loss: 0.7072\n",
            "Epoch 219/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6810 - val_loss: 0.7071\n",
            "Epoch 220/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6809 - val_loss: 0.7069\n",
            "Epoch 221/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.6807 - val_loss: 0.7067\n",
            "Epoch 222/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6805 - val_loss: 0.7066\n",
            "Epoch 223/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6804 - val_loss: 0.7064\n",
            "Epoch 224/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6802 - val_loss: 0.7063\n",
            "Epoch 225/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6801 - val_loss: 0.7062\n",
            "Epoch 226/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6799 - val_loss: 0.7060\n",
            "Epoch 227/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6798 - val_loss: 0.7059\n",
            "Epoch 228/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6796 - val_loss: 0.7058\n",
            "Epoch 229/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6795 - val_loss: 0.7057\n",
            "Epoch 230/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6794 - val_loss: 0.7055\n",
            "Epoch 231/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6792 - val_loss: 0.7054\n",
            "Epoch 232/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6791 - val_loss: 0.7054\n",
            "Epoch 233/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6789 - val_loss: 0.7053\n",
            "Epoch 234/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6788 - val_loss: 0.7052\n",
            "Epoch 235/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6787 - val_loss: 0.7051\n",
            "Epoch 236/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6785 - val_loss: 0.7050\n",
            "Epoch 237/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.6784 - val_loss: 0.7048\n",
            "Epoch 238/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.6782 - val_loss: 0.7046\n",
            "Epoch 239/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6781 - val_loss: 0.7045\n",
            "Epoch 240/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6780 - val_loss: 0.7043\n",
            "Epoch 241/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6778 - val_loss: 0.7040\n",
            "Epoch 242/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.6777 - val_loss: 0.7039\n",
            "Epoch 243/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6775 - val_loss: 0.7038\n",
            "Epoch 244/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6774 - val_loss: 0.7037\n",
            "Epoch 245/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6773 - val_loss: 0.7036\n",
            "Epoch 246/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6771 - val_loss: 0.7035\n",
            "Epoch 247/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6770 - val_loss: 0.7034\n",
            "Epoch 248/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6769 - val_loss: 0.7032\n",
            "Epoch 249/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6767 - val_loss: 0.7031\n",
            "Epoch 250/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.6766 - val_loss: 0.7030\n",
            "Epoch 251/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.6765 - val_loss: 0.7029\n",
            "Epoch 252/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6763 - val_loss: 0.7027\n",
            "Epoch 253/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6762 - val_loss: 0.7026\n",
            "Epoch 254/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6761 - val_loss: 0.7024\n",
            "Epoch 255/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6759 - val_loss: 0.7023\n",
            "Epoch 256/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6758 - val_loss: 0.7022\n",
            "Epoch 257/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6757 - val_loss: 0.7021\n",
            "Epoch 258/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6755 - val_loss: 0.7020\n",
            "Epoch 259/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.6754 - val_loss: 0.7018\n",
            "Epoch 260/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6753 - val_loss: 0.7017\n",
            "Epoch 261/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6751 - val_loss: 0.7016\n",
            "Epoch 262/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6750 - val_loss: 0.7014\n",
            "Epoch 263/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6749 - val_loss: 0.7013\n",
            "Epoch 264/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6747 - val_loss: 0.7012\n",
            "Epoch 265/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6746 - val_loss: 0.7010\n",
            "Epoch 266/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6745 - val_loss: 0.7009\n",
            "Epoch 267/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6743 - val_loss: 0.7007\n",
            "Epoch 268/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6742 - val_loss: 0.7006\n",
            "Epoch 269/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6741 - val_loss: 0.7005\n",
            "Epoch 270/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6739 - val_loss: 0.7004\n",
            "Epoch 271/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6738 - val_loss: 0.7002\n",
            "Epoch 272/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6737 - val_loss: 0.7000\n",
            "Epoch 273/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6735 - val_loss: 0.6998\n",
            "Epoch 274/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6734 - val_loss: 0.6996\n",
            "Epoch 275/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6733 - val_loss: 0.6995\n",
            "Epoch 276/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6732 - val_loss: 0.6993\n",
            "Epoch 277/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6730 - val_loss: 0.6992\n",
            "Epoch 278/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6729 - val_loss: 0.6990\n",
            "Epoch 279/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6728 - val_loss: 0.6989\n",
            "Epoch 280/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6726 - val_loss: 0.6987\n",
            "Epoch 281/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6725 - val_loss: 0.6985\n",
            "Epoch 282/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.6724 - val_loss: 0.6983\n",
            "Epoch 283/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6722 - val_loss: 0.6982\n",
            "Epoch 284/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.6721 - val_loss: 0.6980\n",
            "Epoch 285/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6720 - val_loss: 0.6979\n",
            "Epoch 286/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.6718 - val_loss: 0.6977\n",
            "Epoch 287/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6717 - val_loss: 0.6976\n",
            "Epoch 288/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6716 - val_loss: 0.6974\n",
            "Epoch 289/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6714 - val_loss: 0.6973\n",
            "Epoch 290/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6713 - val_loss: 0.6972\n",
            "Epoch 291/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6712 - val_loss: 0.6970\n",
            "Epoch 292/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6710 - val_loss: 0.6969\n",
            "Epoch 293/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6709 - val_loss: 0.6968\n",
            "Epoch 294/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.6708 - val_loss: 0.6966\n",
            "Epoch 295/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.6706 - val_loss: 0.6965\n",
            "Epoch 296/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6705 - val_loss: 0.6964\n",
            "Epoch 297/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6704 - val_loss: 0.6962\n",
            "Epoch 298/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6702 - val_loss: 0.6960\n",
            "Epoch 299/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6701 - val_loss: 0.6959\n",
            "Epoch 300/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6700 - val_loss: 0.6957\n",
            "Epoch 301/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6698 - val_loss: 0.6955\n",
            "Epoch 302/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6697 - val_loss: 0.6953\n",
            "Epoch 303/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6696 - val_loss: 0.6951\n",
            "Epoch 304/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6694 - val_loss: 0.6949\n",
            "Epoch 305/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6693 - val_loss: 0.6948\n",
            "Epoch 306/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6692 - val_loss: 0.6946\n",
            "Epoch 307/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6690 - val_loss: 0.6944\n",
            "Epoch 308/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6689 - val_loss: 0.6943\n",
            "Epoch 309/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6688 - val_loss: 0.6941\n",
            "Epoch 310/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6686 - val_loss: 0.6939\n",
            "Epoch 311/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6685 - val_loss: 0.6938\n",
            "Epoch 312/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6684 - val_loss: 0.6936\n",
            "Epoch 313/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6682 - val_loss: 0.6934\n",
            "Epoch 314/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6681 - val_loss: 0.6932\n",
            "Epoch 315/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.6680 - val_loss: 0.6931\n",
            "Epoch 316/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6678 - val_loss: 0.6929\n",
            "Epoch 317/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.6677 - val_loss: 0.6927\n",
            "Epoch 318/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.6675 - val_loss: 0.6926\n",
            "Epoch 319/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6674 - val_loss: 0.6924\n",
            "Epoch 320/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6673 - val_loss: 0.6923\n",
            "Epoch 321/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 0.6671 - val_loss: 0.6921\n",
            "Epoch 322/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6670 - val_loss: 0.6920\n",
            "Epoch 323/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6668 - val_loss: 0.6919\n",
            "Epoch 324/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6667 - val_loss: 0.6918\n",
            "Epoch 325/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6666 - val_loss: 0.6917\n",
            "Epoch 326/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6664 - val_loss: 0.6915\n",
            "Epoch 327/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6663 - val_loss: 0.6914\n",
            "Epoch 328/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 0.6661 - val_loss: 0.6913\n",
            "Epoch 329/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6660 - val_loss: 0.6911\n",
            "Epoch 330/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6658 - val_loss: 0.6910\n",
            "Epoch 331/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6657 - val_loss: 0.6908\n",
            "Epoch 332/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6656 - val_loss: 0.6906\n",
            "Epoch 333/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6654 - val_loss: 0.6905\n",
            "Epoch 334/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6653 - val_loss: 0.6904\n",
            "Epoch 335/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6651 - val_loss: 0.6902\n",
            "Epoch 336/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6650 - val_loss: 0.6901\n",
            "Epoch 337/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6648 - val_loss: 0.6900\n",
            "Epoch 338/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.6647 - val_loss: 0.6898\n",
            "Epoch 339/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6646 - val_loss: 0.6897\n",
            "Epoch 340/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6644 - val_loss: 0.6896\n",
            "Epoch 341/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6643 - val_loss: 0.6895\n",
            "Epoch 342/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.6641 - val_loss: 0.6894\n",
            "Epoch 343/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6640 - val_loss: 0.6892\n",
            "Epoch 344/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6638 - val_loss: 0.6891\n",
            "Epoch 345/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6637 - val_loss: 0.6889\n",
            "Epoch 346/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6636 - val_loss: 0.6887\n",
            "Epoch 347/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6634 - val_loss: 0.6886\n",
            "Epoch 348/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6633 - val_loss: 0.6884\n",
            "Epoch 349/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6631 - val_loss: 0.6883\n",
            "Epoch 350/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6630 - val_loss: 0.6881\n",
            "Epoch 351/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.6628 - val_loss: 0.6880\n",
            "Epoch 352/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6627 - val_loss: 0.6878\n",
            "Epoch 353/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6625 - val_loss: 0.6877\n",
            "Epoch 354/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6624 - val_loss: 0.6876\n",
            "Epoch 355/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6623 - val_loss: 0.6875\n",
            "Epoch 356/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.6621 - val_loss: 0.6874\n",
            "Epoch 357/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6620 - val_loss: 0.6873\n",
            "Epoch 358/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6618 - val_loss: 0.6872\n",
            "Epoch 359/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6617 - val_loss: 0.6871\n",
            "Epoch 360/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6615 - val_loss: 0.6870\n",
            "Epoch 361/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6614 - val_loss: 0.6868\n",
            "Epoch 362/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6612 - val_loss: 0.6867\n",
            "Epoch 363/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6611 - val_loss: 0.6865\n",
            "Epoch 364/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6609 - val_loss: 0.6864\n",
            "Epoch 365/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6608 - val_loss: 0.6862\n",
            "Epoch 366/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6606 - val_loss: 0.6860\n",
            "Epoch 367/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6605 - val_loss: 0.6858\n",
            "Epoch 368/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6603 - val_loss: 0.6856\n",
            "Epoch 369/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6602 - val_loss: 0.6855\n",
            "Epoch 370/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6601 - val_loss: 0.6853\n",
            "Epoch 371/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6599 - val_loss: 0.6852\n",
            "Epoch 372/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6598 - val_loss: 0.6850\n",
            "Epoch 373/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6596 - val_loss: 0.6849\n",
            "Epoch 374/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6595 - val_loss: 0.6848\n",
            "Epoch 375/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6593 - val_loss: 0.6847\n",
            "Epoch 376/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6592 - val_loss: 0.6846\n",
            "Epoch 377/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6590 - val_loss: 0.6844\n",
            "Epoch 378/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6589 - val_loss: 0.6843\n",
            "Epoch 379/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6587 - val_loss: 0.6842\n",
            "Epoch 380/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6586 - val_loss: 0.6840\n",
            "Epoch 381/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6584 - val_loss: 0.6839\n",
            "Epoch 382/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6583 - val_loss: 0.6837\n",
            "Epoch 383/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6581 - val_loss: 0.6836\n",
            "Epoch 384/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6580 - val_loss: 0.6835\n",
            "Epoch 385/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6578 - val_loss: 0.6833\n",
            "Epoch 386/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6577 - val_loss: 0.6832\n",
            "Epoch 387/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6575 - val_loss: 0.6830\n",
            "Epoch 388/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6574 - val_loss: 0.6829\n",
            "Epoch 389/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6572 - val_loss: 0.6828\n",
            "Epoch 390/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6571 - val_loss: 0.6826\n",
            "Epoch 391/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6569 - val_loss: 0.6825\n",
            "Epoch 392/500\n",
            "1/1 [==============================] - 0s 29ms/step - loss: 0.6568 - val_loss: 0.6823\n",
            "Epoch 393/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6566 - val_loss: 0.6822\n",
            "Epoch 394/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6564 - val_loss: 0.6821\n",
            "Epoch 395/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6563 - val_loss: 0.6819\n",
            "Epoch 396/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6561 - val_loss: 0.6818\n",
            "Epoch 397/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6560 - val_loss: 0.6816\n",
            "Epoch 398/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6558 - val_loss: 0.6815\n",
            "Epoch 399/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6557 - val_loss: 0.6813\n",
            "Epoch 400/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6555 - val_loss: 0.6811\n",
            "Epoch 401/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6554 - val_loss: 0.6810\n",
            "Epoch 402/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6552 - val_loss: 0.6808\n",
            "Epoch 403/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.6551 - val_loss: 0.6807\n",
            "Epoch 404/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6549 - val_loss: 0.6806\n",
            "Epoch 405/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6547 - val_loss: 0.6805\n",
            "Epoch 406/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6546 - val_loss: 0.6804\n",
            "Epoch 407/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6544 - val_loss: 0.6803\n",
            "Epoch 408/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6543 - val_loss: 0.6801\n",
            "Epoch 409/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6541 - val_loss: 0.6800\n",
            "Epoch 410/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6539 - val_loss: 0.6798\n",
            "Epoch 411/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6538 - val_loss: 0.6796\n",
            "Epoch 412/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6536 - val_loss: 0.6795\n",
            "Epoch 413/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6534 - val_loss: 0.6793\n",
            "Epoch 414/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6533 - val_loss: 0.6791\n",
            "Epoch 415/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6531 - val_loss: 0.6789\n",
            "Epoch 416/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6529 - val_loss: 0.6787\n",
            "Epoch 417/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.6528 - val_loss: 0.6786\n",
            "Epoch 418/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.6526 - val_loss: 0.6785\n",
            "Epoch 419/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6524 - val_loss: 0.6783\n",
            "Epoch 420/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6523 - val_loss: 0.6781\n",
            "Epoch 421/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6521 - val_loss: 0.6779\n",
            "Epoch 422/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6519 - val_loss: 0.6778\n",
            "Epoch 423/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6517 - val_loss: 0.6776\n",
            "Epoch 424/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6516 - val_loss: 0.6774\n",
            "Epoch 425/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6514 - val_loss: 0.6772\n",
            "Epoch 426/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6512 - val_loss: 0.6771\n",
            "Epoch 427/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6510 - val_loss: 0.6770\n",
            "Epoch 428/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6509 - val_loss: 0.6768\n",
            "Epoch 429/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6507 - val_loss: 0.6766\n",
            "Epoch 430/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6505 - val_loss: 0.6765\n",
            "Epoch 431/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 0.6503 - val_loss: 0.6763\n",
            "Epoch 432/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6502 - val_loss: 0.6761\n",
            "Epoch 433/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6500 - val_loss: 0.6759\n",
            "Epoch 434/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6498 - val_loss: 0.6757\n",
            "Epoch 435/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6496 - val_loss: 0.6755\n",
            "Epoch 436/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6495 - val_loss: 0.6754\n",
            "Epoch 437/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6493 - val_loss: 0.6752\n",
            "Epoch 438/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6491 - val_loss: 0.6750\n",
            "Epoch 439/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6489 - val_loss: 0.6748\n",
            "Epoch 440/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6488 - val_loss: 0.6746\n",
            "Epoch 441/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6486 - val_loss: 0.6744\n",
            "Epoch 442/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6484 - val_loss: 0.6742\n",
            "Epoch 443/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6482 - val_loss: 0.6740\n",
            "Epoch 444/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6480 - val_loss: 0.6738\n",
            "Epoch 445/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6479 - val_loss: 0.6736\n",
            "Epoch 446/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6477 - val_loss: 0.6735\n",
            "Epoch 447/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6475 - val_loss: 0.6733\n",
            "Epoch 448/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6473 - val_loss: 0.6731\n",
            "Epoch 449/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6471 - val_loss: 0.6730\n",
            "Epoch 450/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6470 - val_loss: 0.6728\n",
            "Epoch 451/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6468 - val_loss: 0.6726\n",
            "Epoch 452/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6466 - val_loss: 0.6724\n",
            "Epoch 453/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6464 - val_loss: 0.6722\n",
            "Epoch 454/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6462 - val_loss: 0.6721\n",
            "Epoch 455/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6460 - val_loss: 0.6719\n",
            "Epoch 456/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6458 - val_loss: 0.6717\n",
            "Epoch 457/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6457 - val_loss: 0.6714\n",
            "Epoch 458/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6455 - val_loss: 0.6713\n",
            "Epoch 459/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6453 - val_loss: 0.6711\n",
            "Epoch 460/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6451 - val_loss: 0.6709\n",
            "Epoch 461/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6449 - val_loss: 0.6708\n",
            "Epoch 462/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6447 - val_loss: 0.6706\n",
            "Epoch 463/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6445 - val_loss: 0.6704\n",
            "Epoch 464/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 0.6443 - val_loss: 0.6702\n",
            "Epoch 465/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6441 - val_loss: 0.6701\n",
            "Epoch 466/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6439 - val_loss: 0.6698\n",
            "Epoch 467/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6437 - val_loss: 0.6697\n",
            "Epoch 468/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6435 - val_loss: 0.6695\n",
            "Epoch 469/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6433 - val_loss: 0.6692\n",
            "Epoch 470/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6431 - val_loss: 0.6691\n",
            "Epoch 471/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6429 - val_loss: 0.6689\n",
            "Epoch 472/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6427 - val_loss: 0.6687\n",
            "Epoch 473/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6425 - val_loss: 0.6685\n",
            "Epoch 474/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6423 - val_loss: 0.6684\n",
            "Epoch 475/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6421 - val_loss: 0.6682\n",
            "Epoch 476/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6419 - val_loss: 0.6680\n",
            "Epoch 477/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6417 - val_loss: 0.6678\n",
            "Epoch 478/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6415 - val_loss: 0.6676\n",
            "Epoch 479/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6413 - val_loss: 0.6674\n",
            "Epoch 480/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6411 - val_loss: 0.6672\n",
            "Epoch 481/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6409 - val_loss: 0.6670\n",
            "Epoch 482/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6407 - val_loss: 0.6668\n",
            "Epoch 483/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6405 - val_loss: 0.6666\n",
            "Epoch 484/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6403 - val_loss: 0.6664\n",
            "Epoch 485/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.6401 - val_loss: 0.6662\n",
            "Epoch 486/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6399 - val_loss: 0.6660\n",
            "Epoch 487/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.6397 - val_loss: 0.6658\n",
            "Epoch 488/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6395 - val_loss: 0.6656\n",
            "Epoch 489/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6393 - val_loss: 0.6655\n",
            "Epoch 490/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.6391 - val_loss: 0.6653\n",
            "Epoch 491/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6389 - val_loss: 0.6651\n",
            "Epoch 492/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6387 - val_loss: 0.6649\n",
            "Epoch 493/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6385 - val_loss: 0.6648\n",
            "Epoch 494/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6383 - val_loss: 0.6645\n",
            "Epoch 495/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.6381 - val_loss: 0.6643\n",
            "Epoch 496/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6379 - val_loss: 0.6642\n",
            "Epoch 497/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6377 - val_loss: 0.6639\n",
            "Epoch 498/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.6375 - val_loss: 0.6637\n",
            "Epoch 499/500\n",
            "1/1 [==============================] - 0s 25ms/step - loss: 0.6373 - val_loss: 0.6636\n",
            "Epoch 500/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.6371 - val_loss: 0.6634\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "0.618524833688928\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frHhBATWO4cE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictnn2=m.predict(X_test2)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLif_d5DR0aj",
        "colab_type": "text"
      },
      "source": [
        "**Linear Regressor**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRfxKwjlR9iw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "lrmodel2=LinearRegression()\n",
        "lrmodel2.fit(X_train2,y_train2)\n",
        "predictlr2 = lrmodel2.predict(X_test2) # training linear regressor"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEKxEGQXgAV4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "2f1dfd60-948d-4bb5-c608-ec84a214a6a0"
      },
      "source": [
        "print(sklearn.metrics.r2_score(y_test2,lrmodel2.predict(X_test2)))\n",
        "print(sklearn.metrics.r2_score(y_train2,lrmodel2.predict(X_train2)))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6263968841474642\n",
            "0.6503991697541729\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "c1B_Ob3TRaeC"
      },
      "source": [
        "**Support Vector Regressor**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nFRAksARmDC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import SVR\n",
        "svrmodel2=SVR()\n",
        "svrmodel2.fit(X_train2,y_train2)\n",
        "predictsvr2 = svrmodel2.predict(X_test2) # training support vector regressor"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hs1BmmXnfsn0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "ddfd2080-3609-4880-f724-7bb318eebcdd"
      },
      "source": [
        "print(sklearn.metrics.r2_score(y_test2,svrmodel2.predict(X_test2)))\n",
        "print(sklearn.metrics.r2_score(y_train2,svrmodel2.predict(X_train2)))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6170843051045032\n",
            "0.6848597167841399\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvwNXNdjVl5R",
        "colab_type": "text"
      },
      "source": [
        "KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8hTI2suVqBw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "knnmodel2=KNeighborsRegressor()\n",
        "knnmodel2.fit(X_train2,y_train2)\n",
        "predictknn2 = knnmodel2.predict(X_test2) # training KNN regressor"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFzNW9gRfdbZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "a64a1301-0d27-4069-e6d7-82258243bc3a"
      },
      "source": [
        "print(sklearn.metrics.r2_score(y_test2,knnmodel2.predict(X_test2)))\n",
        "print(sklearn.metrics.r2_score(y_train2,knnmodel2.predict(X_train2)))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5895647484820485\n",
            "0.7390600504072775\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Avx0nrqOV4sX",
        "colab_type": "text"
      },
      "source": [
        "**ADABOOST**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LG2X9vX2V-T4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "adbmodel2=AdaBoostRegressor()\n",
        "adbmodel2.fit(X_train2,y_train2)\n",
        "predictadb2 = adbmodel2.predict(X_test2) # training AdaBoost regressor\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owko77YzfChq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "7d59322c-8320-4393-f78a-39125f9ba03f"
      },
      "source": [
        "print(sklearn.metrics.r2_score(y_test2,adbmodel2.predict(X_test2)))\n",
        "print(sklearn.metrics.r2_score(y_train2,adbmodel2.predict(X_train2)))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5616650994003165\n",
            "0.5918717505900204\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtzTLUaCorXO",
        "colab_type": "text"
      },
      "source": [
        "**Random Forest**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ne_URAzMQy86",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "RFmodel2 = RandomForestRegressor(n_estimators=900)\n",
        "RFmodel2.fit(X_train2,y_train2) # making random forest regressor\n",
        "predictrf2 = RFmodel2.predict(X_test2) "
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTPhYshnevJF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "4b8a3e1f-7679-4cb8-e7df-5dc8e89dda6e"
      },
      "source": [
        "print(sklearn.metrics.r2_score(y_test2,RFmodel2.predict(X_test2))) \n",
        "print(sklearn.metrics.r2_score(y_train2,RFmodel2.predict(X_train2))) "
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6672925609843141\n",
            "0.9443790983147142\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mX9BqDtlzil3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "6b37707e-8930-4932-fc8e-a460095b9193"
      },
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "# Number of trees in random forest\n",
        "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
        "# Number of features to consider at every split\n",
        "max_features = ['auto', 'sqrt']\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
        "max_depth.append(None)\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [2, 5, 10]\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [1, 2, 4]\n",
        "# Method of selecting samples for training each tree\n",
        "bootstrap = [True, False]\n",
        "# Create the random grid\n",
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf,\n",
        "               'bootstrap': bootstrap}\n",
        "print(random_grid)\n",
        "{'bootstrap': [True, False],\n",
        " 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
        " 'max_features': ['auto', 'sqrt'],\n",
        " 'min_samples_leaf': [1, 2, 4],\n",
        " 'min_samples_split': [2, 5, 10],\n",
        " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bootstrap': [True, False],\n",
              " 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
              " 'max_features': ['auto', 'sqrt'],\n",
              " 'min_samples_leaf': [1, 2, 4],\n",
              " 'min_samples_split': [2, 5, 10],\n",
              " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcfBS0QNz50F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "b5349ec6-f7c8-4ede-fdbc-6adcad55e30f"
      },
      "source": [
        "# Use the random grid to search for best hyperparameters\n",
        "# First create the base model to tune\n",
        "rf = RandomForestRegressor()\n",
        "# Random search of parameters, using 3 fold cross validation, \n",
        "# search across 100 different combinations, and use all available cores\n",
        "rf_random2 = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
        "# Fit the random search model\n",
        "rf_random2.fit(X_train2,y_train2) # making random forest regressor\n",
        "predict = rf_random2.predict(X_test2) "
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  1.4min\n",
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "[Parallel(n_jobs=-1)]: Done 158 tasks      | elapsed:  6.1min\n",
            "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed: 11.3min finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8xZO2pRY7Km",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "911bbd00-bd8f-473e-8e1c-65ae67259334"
      },
      "source": [
        "print(sklearn.metrics.r2_score(y_test2,rf_random2.predict(X_test2)))\n",
        "print(sklearn.metrics.r2_score(y_train2,rf_random2.predict(X_train2)))\n",
        "predictrfrand2 = rf_random2.predict(X_test2)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6836525168721266\n",
            "0.9469109396348587\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7WtS5a1Vo3n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.externals import joblib\n",
        "joblib.dump(rf_random2, \"rf_A_optimized.pkl\") \n",
        "my_model_loaded = joblib.load(\"rf_A_optimized.pkl\")"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJCuwQV7X7Ye",
        "colab_type": "text"
      },
      "source": [
        "Gradient Boosting Regressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Xum7BRkRYB9z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e41f2ab2-bec3-404c-c1a0-f1b5b5dc9c0d"
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "gbrmodel2=GradientBoostingRegressor()\n",
        "gbrmodel2.fit(X_train2,y_train2)\n",
        "predictgbr2 = gbrmodel2.predict(X_test2) # training Gradient Boosting regressor\n",
        "print(sklearn.metrics.r2_score(y_test2,gbrmodel2.predict(X_test2))) # r2 score of Gradient Boosting regressor\n",
        "print(sklearn.metrics.r2_score(y_train2,gbrmodel2.predict(X_train2)))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6289202840307022\n",
            "0.6993573583939822\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4sXFqcupLeb",
        "colab_type": "text"
      },
      "source": [
        "**Using our models to make a Custom Prediction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPm4RiIQ2_Gd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "outputId": "5f240f09-5a3a-49ef-ceba-f87696eec3b6"
      },
      "source": [
        "stri = input() # input your custom grades \n",
        "arr = to_arr(stri)\n",
        "RFmodel.predict([arr])[0] # making our first custom prediction"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    490\u001b[0m         \"\"\"\n\u001b[0;32m--> 491\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-d8722172ff4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstri\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# input your custom grades\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_arr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mRFmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# making our first custom prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-mytrhZEwEE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cde5bb83-d506-43ef-9e69-871bf89cbaf3"
      },
      "source": [
        "pr=(predictlr2+predictgbr2+predictsvr2+predictrfrand2)/4\n",
        "sklearn.metrics.r2_score(y_test2,pr)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6563520634642175"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80HkJsO8OJ7T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "5d90e983-ca68-4bc2-a897-37cdbb3c5e4a"
      },
      "source": [
        "df=pd.DataFrame()\n",
        "df['f1']=predictlr2\n",
        "df['f2']=predictgbr2\n",
        "df['f3']=predictsvr2\n",
        "df['f4']=predictrfrand2\n",
        "df['f5']=predictnn2\n",
        "df['l']=y_test2\n",
        "df.head()"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f1</th>\n",
              "      <th>f2</th>\n",
              "      <th>f3</th>\n",
              "      <th>f4</th>\n",
              "      <th>f5</th>\n",
              "      <th>l</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4.746059</td>\n",
              "      <td>4.834714</td>\n",
              "      <td>4.497220</td>\n",
              "      <td>4.404706</td>\n",
              "      <td>4.435067</td>\n",
              "      <td>4.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6.951979</td>\n",
              "      <td>7.075814</td>\n",
              "      <td>6.908167</td>\n",
              "      <td>7.183721</td>\n",
              "      <td>7.011200</td>\n",
              "      <td>6.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8.216078</td>\n",
              "      <td>8.461642</td>\n",
              "      <td>8.428119</td>\n",
              "      <td>8.614707</td>\n",
              "      <td>8.299201</td>\n",
              "      <td>8.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6.604057</td>\n",
              "      <td>6.303044</td>\n",
              "      <td>6.675528</td>\n",
              "      <td>6.900967</td>\n",
              "      <td>6.638798</td>\n",
              "      <td>6.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8.048497</td>\n",
              "      <td>8.051295</td>\n",
              "      <td>8.123513</td>\n",
              "      <td>7.882732</td>\n",
              "      <td>8.044033</td>\n",
              "      <td>7.4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         f1        f2        f3        f4        f5    l\n",
              "0  4.746059  4.834714  4.497220  4.404706  4.435067  4.1\n",
              "1  6.951979  7.075814  6.908167  7.183721  7.011200  6.1\n",
              "2  8.216078  8.461642  8.428119  8.614707  8.299201  8.6\n",
              "3  6.604057  6.303044  6.675528  6.900967  6.638798  6.8\n",
              "4  8.048497  8.051295  8.123513  7.882732  8.044033  7.4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Kpv6tn3PpWe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "ebab0d63-22ee-4ac9-a487-007e1d7d2a53"
      },
      "source": [
        "x=df.iloc[:,:-1]\n",
        "y=df.iloc[:,-1:]\n",
        "y.head()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>l</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6.8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7.4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     l\n",
              "0  4.1\n",
              "1  6.1\n",
              "2  8.6\n",
              "3  6.8\n",
              "4  7.4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91lVQvdWPej5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a0c6a88c-4f56-41d6-8450-64dbe1c6e5d4"
      },
      "source": [
        "x1, x2, y1, y2= train_test_split(x,y,test_size=0.4,random_state=101)\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lrmodel3=LinearRegression()\n",
        "lrmodel3.fit(x1,y1)\n",
        "sklearn.metrics.r2_score(y2,lrmodel3.predict(x2))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7192085660828655"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    }
  ]
}